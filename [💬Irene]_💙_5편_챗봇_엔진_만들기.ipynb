{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPrJUoTPLVhx"
      },
      "source": [
        "# 챗봇 엔진 만들기\n",
        "\n",
        "## 들어가기 전에\n",
        "\n",
        "**챗봇 엔진**이란 챗봇에서 핵심 기능을 하는 자연어 처리 모듈로, 화자의 질문을 이해하고 알맞은 답변을 출력하는 역할을 합니다.\n",
        "\n",
        "챗봇 엔진을 설계하기 전에는 챗봇의 목적과 도메인을 결정해야 합니다. 이에 따라 챗봇 엔진 개발 방법론과 학습에 필요한 데이터셋이 달라집니다. 이 글의 예시에서는 음식 예약 및 주문을 도와주는 챗봇 엔진을 만들겠습니다.\n",
        "\n",
        "챗봇 엔진의 처리 과정은 다음 네 단계로 나타낼 수 있습니다.\n",
        "\n",
        "![챗봇 엔진의 처리 과정](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/2a84acfc-7a2e-4819-8d86-bf59d11f2b5f/Screen_Shot_2022-08-05_at_6.17.45_AM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220809%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220809T011806Z&X-Amz-Expires=86400&X-Amz-Signature=bea2185c994fbd80aeb6f9a6d0cb6359ef3cfc1de8e75871059a44d0b3ad3e1c&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Screen%2520Shot%25202022-08-05%2520at%25206.17.45%2520AM.png%22&x-id=GetObject)\n",
        "\n",
        "1. **전처리 과정**: 화자의 질의 문장이 입력되면 챗봇 엔진은 제일 먼저 전처리를 진행합니다. 형태소 분석기를 이용해 단어 토큰을 추출한 뒤 명사나 동사 등 문장 해석에 의미 있는 품사만 남기고 불용어는 제거합니다.\n",
        "2. **질문 의도 분류**: 화자의 질문 의도를 파악합니다. 즉 의도 분류 모델을 이용해 의도 클래스를 예측합니다.\n",
        "3. **개체명 인식**: 화자의 질문에서 단어 토큰별 개체명을 인식합니다.\n",
        "4. **답변 검색**: 해당 질문의 의도, 개체명, 핵심 키워드를 기반으로 답변을 학습 DB에서 검색합니다.\n",
        "\n",
        "앞으로의 글은 위 네 단계를 순차적으로 다룰 것입니다. 추가적으로 의도 분류 및 개체명 인식 모델의 학습을 하려면 단어 사전을 구축해야 하기 때문에 **단어 사전 구축 및 시퀀스 생성 방법**을 알아볼 것입니다. 또한 다양한 플랫폼에서 우리가 만든 챗봇 엔진에 언제든 접속할 수 있도록 **챗봇 엔진 서버 프로그램 개발**에 대한 내용도 살피도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yLGvnfhOkNX",
        "outputId": "5818bbe9-2ca5-4415-ee97-d8c2c795115f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'chatbot'...\n",
            "remote: Enumerating objects: 225, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 225 (delta 7), reused 5 (delta 5), pack-reused 216\u001b[K\n",
            "Receiving objects: 100% (225/225), 90.83 MiB | 24.55 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/keiraydev/chatbot.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucy_XBMcOpTZ",
        "outputId": "6322237b-172f-41d8-efcf-982feae5baf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/chatbot\n"
          ]
        }
      ],
      "source": [
        "%cd chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMBkU19mN45h",
        "outputId": "3f840795-955f-4005-e382-d7394ecdabbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 92.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=ffcb5f56a3547ff02dc7c32648583475729f06a2b0274ba0201aac1ee0c08e5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymysql\n",
            "  Downloading PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pymysql\n",
            "Successfully installed pymysql-1.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install konlpy \n",
        "!pip install tensorflow\n",
        "!pip install seqeval\n",
        "!pip install pymysql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NHHEfZ5L0MB"
      },
      "source": [
        "## 1. 전처리 과정\n",
        "\n",
        "전처리를 담당하는 모듈을 만들어봅시다. /utils 디렉터리에 Preprocess.py라는 파일을 생성하도록 하겠습니다. 챗봇 엔진 내에서 자주 사용하기 때문에 클래스로 정의합니다. 전체적인 디렉터리 구조는 [4편 게시글](https://dacon.io/competitions/official/235946/codeshare/5863)에 소개되어 있습니다.\n",
        "\n",
        "`pos` 함수는 형태소 분석기로 토크나이징 작업을 담당하고, `get_keywords` 함수는 문장 해석에 의미 있는 정보만 남기고 나머지 불용어들은 제거하는 작업을 합니다. 토크나이징에 대한 더 자세한 내용은 [1편 게시글](https://dacon.io/competitions/official/235946/codeshare/5539?page=1&dtype=random)을 참고해주세요. 마지막으로 `get_wordidx_sequence` 함수는 다음 목차에서 구축할 단어 사전을 이용해 입력된 문장을 단어 시퀀스 벡터로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bScePPHNtMZ"
      },
      "outputs": [],
      "source": [
        "from konlpy.tag import Komoran\n",
        "import pickle\n",
        "\n",
        "class Preprocess:\n",
        "\t# 생성자\n",
        "\tdef __init__(self, word2index_dic=\"\", userdic=None): \n",
        "\t\t# 단어 인덱스 사전 불러오기\n",
        "\t\tif word2index_dic != \"\":\n",
        "\t\t\tf = open(word2index_dic, \"rb\")\n",
        "\t\t\tself.word_index = pickle.load(f)\n",
        "\t\t\tf.close()\n",
        "\t\telse:\n",
        "\t\t\tself.word_index = None\n",
        "\n",
        "\t\t# 형태소 분석기 초기화\n",
        "\t\tself.komoran = Komoran(userdic=userdic)\n",
        "\n",
        "\t\t# 제외할 품사\n",
        "\t\t# 참조: https://docs.komoran.kr/firststep/postypes.html\n",
        "\t\tself.exclusion_tags = [\n",
        "\t\t\t\"JKS\", \"JKC\", \"JKG\", \"JKO\", \"JKB\", \"JKV\", \"JKQ\", \"JX\", \"JC\", # 관계언 제거\n",
        "\t\t\t\"SF\", \"SP\", \"SS\", \"SE\", \"SO\", # 기호 제거\n",
        "\t\t\t\"EP\", \"EF\", \"EC\", \"ETN\", \"ETM\", # 어미 제거\n",
        "\t\t\t\"XSN\", \"XSV\", \"XSA\", # 접미사 제거 \n",
        "\t\t]\n",
        "\n",
        "\t# 형태소 분석기 POS tagger (래퍼 함수)\n",
        "\tdef pos(self, sentence):\n",
        "\t\treturn self.komoran.pos(sentence)\n",
        "\n",
        "\t# 불용어 제거 후 필요한 품사 정보만 가져오기\n",
        "\tdef get_keywords(self, pos, without_tag=False):\n",
        "\t\tf = lambda x: x in self.exclusion_tags\n",
        "\t\tword_list = []\n",
        "\t\tfor p in pos:\n",
        "\t\t\tif f(p[1]) is False: # 불용어 리스트에 없는 경우에만 저장\n",
        "\t\t\t\tword_list.append(p if without_tag is False else p[0])\n",
        "\t\treturn word_list\n",
        "\n",
        "\t# 키워드를 단어 인덱스 시퀀스로 변환\n",
        "\tdef get_wordidx_sequence(self, keywords):\n",
        "\t\tif self.word_index is None:\n",
        "\t\t\treturn []\n",
        "\t\tw2i = []\n",
        "\t\tfor word in keywords:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tw2i.append(self.word_index[word])\n",
        "\t\t\texcept KeyError:\n",
        "\t\t\t\t# 해당 단어가 사전에 없는 경우 OOV 처리\n",
        "\t\t\t\tw2i.append(self.word_index[\"OOV\"])\n",
        "\t\treturn w2i"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv0NCv7sOIii"
      },
      "source": [
        "## 2. 단어 사전 구축 및 시퀀스 생성\n",
        "\n",
        "/train_tools/dict 디렉터리에 있는 말뭉치 데이터 corpus.txt를 활용하여 단어 사전을 생성해봅시다. 같은 디렉터리에 create_dict.py 파일을 만들도록 하겠습니다. 이 파일에서 생성된 단어 사전은 의도 분류 및 개체명 인식 모델의 학습에 활용될 예정입니다. 단어 사전 구축에 대한 더 자세한 내용 역시 [1편 게시글](https://dacon.io/competitions/official/235946/codeshare/5539?page=1&dtype=random)을 참고해주시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_S3aYzANu92"
      },
      "outputs": [],
      "source": [
        "from utils.Preprocess import Preprocess\n",
        "from tensorflow.keras import preprocessing\n",
        "import pickle\n",
        "\n",
        "# 말뭉치 데이터 읽어오기\n",
        "def read_corpus_data(filename):\n",
        "\twith open(filename, \"r\") as f:\n",
        "\t\tdata = [line.split(\"\\t\") for line in f.read().splitlines()]\n",
        "\t\tdata = data[1:] # 헤더 제거\n",
        "\treturn data\n",
        "\n",
        "# 말뭉치 데이터 가져오기\n",
        "corpus_data = read_corpus_data(\"./train_tools/dict/corpus.txt\")\n",
        "\n",
        "# 말뭉치 데이터에서 키워드만 추출해서 사전 리스트 생성\n",
        "p = Preprocess()\n",
        "dict = []\n",
        "for c in corpus_data:\n",
        "\tpos = p.pos(c[1])\n",
        "\tfor k in pos:\n",
        "\t\tdict.append(k[0])\n",
        "\n",
        "# 사전에 사용될 단어 인덱스 딕셔너리(word_index) 생성\n",
        "tokenizer = preprocessing.text.Tokenizer(oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(dict)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# 사전 파일 생성\n",
        "f = open(\"chatbot_dict.bin\", \"wb\")\n",
        "try:\n",
        "\tpickle.dump(word_index, f)\n",
        "except Exception as e:\n",
        "\tprint(e)\n",
        "finally:\n",
        "\tf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goKAUaSEPyTy"
      },
      "source": [
        "우리가 사용한 단어가 사전에 존재하지 않을 수 있습니다. 이 경우 챗봇 엔진에서는 OOV (Out-of-Vocabulary) 처리를 합니다. 위 코드에서 oov_token을 “OOV”라는 string으로 설정하면 사전의 첫 번째 인덱스에 “OOV”가 저장됩니다. 따라서 나중에 단어 인덱스 시퀀스를 만들 때 어떤 단어가 사전에 없는 경우 “OOV”의 인덱스를 저장하는 식으로 처리해주면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD29q5JfP0Qw"
      },
      "source": [
        "## 3. 의도 분류 모델\n",
        "\n",
        "챗봇 엔진에 화자의 질의가 입력되었을 때, 전처리 과정을 거친 후 해당 문장의 의도를 분류합니다. 문장을 의도 클래스별로 분류하기 위해 이전에 사용했던 모델인 CNN을 사용하겠습니다. 다양한 의도를 분류하기에는 학습 데이터 수가 한정적이기 때문에 5가지 의도로만 분류할 수 있도록 구현하겠습니다.\n",
        "\n",
        "- 챗봇 엔진의 의도 분류 클래스 종류\n",
        "\n",
        "![Untitled](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/e94e9a4b-e938-4d4f-ab81-5601c85b3db3/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220809%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220809T013712Z&X-Amz-Expires=86400&X-Amz-Signature=372239f6339b3af224f229790341357de35519c2f40aa4c81af92cf1326e0735&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
        "\n",
        "모델 학습 모듈을 만들기 전에 챗봇 엔진 소스 전역에서 사용할 파라미터 정보를 /config 디렉터리 내에 파일로 관리하겠습니다. [GlobalParams.py](http://GlobalParams.py) 파일을 생성해 다음과 같이 작성하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iym-Pg6sP-e-"
      },
      "outputs": [],
      "source": [
        "# 단어 시퀀스 벡터 크기\n",
        "MAX_SEQ_LEN =15\n",
        "\n",
        "def GlobalParams():\n",
        "\tglobal MAX_SEQ_LEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E96mndMYP_gE"
      },
      "source": [
        "### 3.1 의도 분류 모델 학습\n",
        "\n",
        "챗봇 엔진의 의도 분류 모듈을 만들기 전에 해당 모델의 설계 및 학습을 진행하겠습니다. \n",
        "\n",
        "/model/intent 디렉터리에 total_train_data.csv 파일을 학습 데이터셋으로 사용합니다. 이 파일은 이전에 사용한 chatbot_data.csv(송영숙 님 공개 데이터)을 기반으로 생성한 의도 분류용 학습 데이터셋입니다. 이 데이터는 음식점 주문과 예약을 위한 챗봇에 특화되어 있으며, 클래스별 샘플 텍스트가 다양하지 않아 특정 의도 클래스인 경우 예측 품질이 떨어질 수 있습니다.\n",
        "\n",
        "다음은 total_train_data.csv 파일을 읽어와 의도 분류 모델을 생성하고 학습하는 코드입니다. \n",
        "\n",
        "해당위치(/models/intent 디렉터리)에 train_model.py 파일을 생성하세요.\n",
        "\n",
        "- 챗봇 엔진 의도 분류 모델\n",
        "\n",
        "total_train_data.csv 파일을 읽어와 CNN 모델 학습 시 필요한 query(질문)과 intent(의도)를 리스트에 저장합니다. 그 다음 챗봇 전처리 모듈 Preprocess로 단어 시퀀스를 생성합니다. 해당 단어에 매칭되는 번호로 시퀀스를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwnhXhyPOh4Y"
      },
      "outputs": [],
      "source": [
        "# 필요한 모듈 임포트\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
        "\n",
        "\n",
        "# 데이터 읽어오기\n",
        "train_file = \"./models/intent/total_train_data.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "queries = data['query'].tolist()\n",
        "intents = data['intent'].tolist()\n",
        "\n",
        "from utils.Preprocess import Preprocess\n",
        "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict.bin',\n",
        "               userdic='./utils/user_dic.tsv')\n",
        "\n",
        "# 단어 시퀀스 생성\n",
        "sequences = []\n",
        "for sentence in queries:\n",
        "    pos = p.pos(sentence)\n",
        "    keywords = p.get_keywords(pos, without_tag=True)\n",
        "    seq = p.get_wordidx_sequence(keywords)\n",
        "    sequences.append(seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tLCwcLPQVeX"
      },
      "source": [
        "위에서 생성한 단어 시퀀스 벡터의 크기를 동일하게 맞춰주기 위해 MAX_SEQ_LEN 크기만큼 시퀀스 벡터를 패딩 처리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_drj0iXQWPE"
      },
      "outputs": [],
      "source": [
        "# 단어 인덱스 시퀀스 벡터 \n",
        "# 단어 시퀀스 벡터 크기\n",
        "from config.GlobalParams import MAX_SEQ_LEN\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I15nEaeOQZiN"
      },
      "source": [
        "패딩 처리된 시퀀스 벡터 리스트와 intent(의도) 리스트 전체를 데이터셋 객체로 만듭니다. 그 다음 데이터를 랜덤으로 섞고 학습용, 검증용, 테스트용 데이터 셋으로 7:2:1 비율로 나눠 실제 학습에 필요한 데이터셋 객체를 각각 분리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU_ZPZvFQbGL"
      },
      "outputs": [],
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
        "ds = ds.shuffle(len(queries))\n",
        "\n",
        "train_size = int(len(padded_seqs) * 0.7)\n",
        "val_size = int(len(padded_seqs) * 0.2)\n",
        "test_size = int(len(padded_seqs) * 0.1)\n",
        "\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRsiJlmxQcoI"
      },
      "source": [
        "케라스 함수형 모델 방식으로 의도 분류 모델을 구현합니다. 입력하는 문장을 의도 클래스로 분류하는 CNN모델은 여러 영역으로 구성되어 있습니다. 첫 번째로 입력 데이터를 단어 임베딩 처리하는 영역, 그 다음으로 합성곱 필터와 연산을 통해  특징맵을 추출하고 평탄화하는 영역, 완전 연결 계층을 통해 감정별로 클래스를 분류하는 영역으로 구성되어 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rQMsWpKQgO0"
      },
      "outputs": [],
      "source": [
        "# 하이퍼 파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 5\n",
        "VOCAB_SIZE = len(p.word_index) + 1 #전체 단어 개수\n",
        "\n",
        "\n",
        "# CNN 모델 정의  ○4\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
        "\n",
        "conv1 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=3,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "\n",
        "conv2 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=4,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "\n",
        "conv3 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=5,\n",
        "    padding='valid',\n",
        "    activation=tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)\n",
        "\n",
        "# 3,4,5gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uywa2m22QlaW"
      },
      "source": [
        "우리는 5가지 의도 클래스를 분류해야하기에 출력노드가 5개인 Dense 계층을 생성합니다. 마지막으로 출력 노드로 정의한 logits에서 나온 함수를 소프트맥스 계층을 통해 감정 클래스별 확률을 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV-EzU0MQjTX"
      },
      "outputs": [],
      "source": [
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "logits = Dense(5, name='logits')(dropout_hidden)\n",
        "predictions = Dense(5, activation=tf.nn.softmax)(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRbDc2nZQp3B"
      },
      "source": [
        "위에서 정의한 계층들을 케라스 모델에 추가하는 작업을 진행합니다. 모델 정의 후 실제 모델을 model.compile() 함수를 통해 CNN모델을 컴파일합니다. 최적화 방법에는 adam, 손실함수에는 sparse_categorical_crossentropy, 모델 평가할 때 정확도 확인하기 위해 metrics에 accuracy를 사용하도록 했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HxtsLKMQnIv"
      },
      "outputs": [],
      "source": [
        "#모델 생성\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WrBGLqcQuMH"
      },
      "source": [
        "모델을 학습하기 위해 model.fit() 함수를 사용합니다. 에포크값을 5로 설정했으므로 모델 학습을 5회 반복합니다. 또한 evaluate() 함수를 이용해 성능을 평가합니다. 인자에는 테스트용 데이터셋을 사용합니다. 마지막으로 학습이 완료된 모델을 h5 파일 포맷으로 저장합니다. 해당 모델 파일은 챗봇 엔진의 의도 분류 모델에서 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kec9D4fDQo_W",
        "outputId": "faf9fa59-fb48-4915-a7f6-d76dfa9f0b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "3698/3698 [==============================] - 86s 23ms/step - loss: 0.0487 - accuracy: 0.9849 - val_loss: 0.0130 - val_accuracy: 0.9954\n",
            "Epoch 2/5\n",
            "3698/3698 [==============================] - 86s 23ms/step - loss: 0.0151 - accuracy: 0.9946 - val_loss: 0.0078 - val_accuracy: 0.9969\n",
            "Epoch 3/5\n",
            "3698/3698 [==============================] - 83s 22ms/step - loss: 0.0116 - accuracy: 0.9957 - val_loss: 0.0060 - val_accuracy: 0.9972\n",
            "Epoch 4/5\n",
            "3698/3698 [==============================] - 84s 23ms/step - loss: 0.0090 - accuracy: 0.9964 - val_loss: 0.0083 - val_accuracy: 0.9940\n",
            "Epoch 5/5\n",
            "3698/3698 [==============================] - 86s 23ms/step - loss: 0.0080 - accuracy: 0.9967 - val_loss: 0.0062 - val_accuracy: 0.9970\n",
            "529/529 [==============================] - 2s 3ms/step - loss: 0.0066 - accuracy: 0.9967\n",
            "Accuracy: 99.668717\n",
            "loss: 0.006590\n"
          ]
        }
      ],
      "source": [
        "# 모델 학습 \n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
        "\n",
        "\n",
        "# 모델 평가(테스트 데이터 셋 이용) \n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy * 100))\n",
        "print('loss: %f' % (loss))\n",
        "\n",
        "\n",
        "# 모델 저장  \n",
        "model.save('intent_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_179GvQQxxw"
      },
      "source": [
        "### 3.2 의도 분류 모듈 생성\n",
        "\n",
        "이제 챗봇 엔진의 의도 분류 모듈을 만들겠습니다. 이 모듈은 앞서 학습한 의도 분류 모델 파일을 활용해 입력되는 텍스트의 의도 클래스를 예측하는 기능을 가지고 있습니다. 해당 모듈은 딥러닝 모델이기에 /model/intent 디렉터리 내에 intentModel.py로 생성해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyhLZ256QvaA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "\n",
        "# 의도 분류 모델 모듈\n",
        "class IntentModel:\n",
        "    def __init__(self, model_name, proprocess):\n",
        "\n",
        "        # 의도 클래스 별 레이블\n",
        "        self.labels = {0: \"인사\", 1: \"욕설\", 2: \"주문\", 3: \"예약\", 4: \"기타\"}\n",
        "\n",
        "        # 의도 분류 모델 불러오기\n",
        "        self.model = load_model(model_name)\n",
        "\n",
        "        # 챗봇 Preprocess 객체\n",
        "        self.p = proprocess\n",
        "\n",
        "\n",
        "    # 의도 클래스 예측\n",
        "    def predict_class(self, query):\n",
        "        # 형태소 분석\n",
        "        pos = self.p.pos(query)\n",
        "\n",
        "        # 문장내 키워드 추출(불용어 제거)\n",
        "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
        "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
        "\n",
        "        # 단어 시퀀스 벡터 크기\n",
        "        from config.GlobalParams import MAX_SEQ_LEN\n",
        "\n",
        "        # 패딩처리\n",
        "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "        predict = self.model.predict(padded_seqs)\n",
        "        predict_class = tf.math.argmax(predict, axis=1)\n",
        "        return predict_class.numpy()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OuB6CQ4Qz6N"
      },
      "source": [
        "다음은 IntentModel 클래스를 테스트하는 코드입니다. IntentModel 객체를 생성해 새로운 유형의 문장을 분류합니다. 테스트 코드이므로 /test 디렉터리에 model_intent_test.py로 생성해주세요. predict는 query에 대한 예측 클래스 정보를 나타내고 predict_label은 query에 대한 예측 레이블을 나타냅니다. 학습 데이터에서 크게 벗어나지 않는 문장의 경우 의도를 적절하게 잘 예측합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT_QYUhWQ1oE"
      },
      "outputs": [],
      "source": [
        "from utils.Preprocess import Preprocess\n",
        "from models.intent.IntentModel import IntentModel\n",
        "\n",
        "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict.bin',\n",
        "               userdic='../utils/user_dic.tsv')\n",
        "\n",
        "intent = IntentModel(model_name='./models/intent/intent_model.h5', proprocess=p)\n",
        "query = \"오늘 탕수육 주문 가능한가요?\"\n",
        "predict = intent.predict_class(query)\n",
        "predict_label = intent.labels[predict]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqTN0ImGQ3ET"
      },
      "source": [
        "## 4. 개체명 인식 모델 학습\n",
        "\n",
        "다음으로 챗봇 엔진에 입력된 문장 의도 분류 후, 문장 내 개체명 인식(Named Entity Regocnition, NER) 모델을 만들어봅시다. 개체명 인식을 위해 지난 [3-1편에서 배운 양방향 LSTM](https://dacon.io/competitions/official/235946/codeshare/5812) 모델을 사용해보도록 하겠습니다.\n",
        "\n",
        "개체명 인식 모델에서 인식 가능한 주요 개체명은 다음과 같습니다.\n",
        "\n",
        "- **개체명 종류**\n",
        "\n",
        "![Untitled](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/bfce0010-a2c8-46ac-8358-4e29c6c59e48/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220809%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220809T014144Z&X-Amz-Expires=86400&X-Amz-Signature=c7a340d0d95dac53e014bdb69be591703fc0dd79dfeaf0ec6d5a3a6932832847&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
        "\n",
        "### 4.1 개체명 인식 모델 학습\n",
        "\n",
        "/models/ner 디렉터리의 ner_train.txt 파일을 학습 데이터셋으로 사용합니다. 해당 파일은 지난 3편에서 사용한 train.txt을 기반으로 저자가 기존 학습 데이터셋에서 음식, 날짜, 시간 BIO 태그 데이터를 보강하여 생성한 개체명 인식용 학습 데이터셋입니다.\n",
        "\n",
        "/models/ner 디렉터리 위치에서 train_model.py 파일을 생성하여 ner_train.txt 파일을 읽어와 NER 모델을 생성하고 학습하는 코드를 작성해보겠습니다.\n",
        "\n",
        "(모델 학습 데이터 처리 부분만 다를 뿐 지난 3-1편에서의 개체명 인식 모델과 거의 동일한 소스 코드입니다.)\n",
        "\n",
        "- **챗봇 엔진 NER 모델**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygJYQw48Q71Q"
      },
      "outputs": [],
      "source": [
        "# 학습 파일 불러오기\n",
        "def read_file(file_name):\n",
        "    sents = []\n",
        "    with open(file_name, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for idx, l in enumerate(lines):\n",
        "            if l[0] == ';' and lines[idx + 1][0] == '$':\n",
        "                this_sent = []\n",
        "            elif l[0] == '$' and lines[idx - 1][0] == ';':\n",
        "                continue\n",
        "            elif l[0] == '\\n':\n",
        "                sents.append(this_sent)\n",
        "            else:\n",
        "                this_sent.append(tuple(l.split()))\n",
        "\n",
        "    return sents\n",
        "\n",
        "# 전처리 객체 생성\n",
        "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict.bin',\n",
        "                  userdic = './utils/user_dic.tsv')\n",
        "\n",
        "# 학습용 말뭉치 데이터 불러옴\n",
        "corpus = read_file('./models/ner/ner_train.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lB9cb3VRBzj"
      },
      "source": [
        "개체 인식 모델을 학습하기 위한 학습용 말뭉치 데이터를 read_file 함수를 통해 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8jVzMszRDnr",
        "outputId": "74c534dd-f259-4501-e264-75467403b9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "샘플 크기 : \n",
            " 61999\n",
            "0번째 샘플 단어 시퀀스 : \n",
            " ['가락지빵', '주문', '하', '고', '싶', '어요']\n",
            "0번째 샘플 bio 태그 : \n",
            " ['B_FOOD', 'O', 'O', 'O', 'O', 'O']\n",
            "샘플 단어 시퀀스 최대 길이 : \n",
            " 168\n",
            "샘플 단어 시퀀스 평균 길이 : \n",
            " 8.796238649010467\n"
          ]
        }
      ],
      "source": [
        "# 말뭉치 데이터에서 단어와 BIO 태그만 불러와 학습용 데이터셋 생성\n",
        "sentences, tags = [], []\n",
        "for t in corpus:\n",
        "  tagged_sentence = []\n",
        "  sentence, bio_tag = [], []\n",
        "  for w in t:\n",
        "      tagged_sentence.append((w[1], w[3]))\n",
        "      sentence.append(w[1])\n",
        "      bio_tag.append(w[3])\n",
        "\n",
        "  sentences.append(sentence)\n",
        "  tags.append(bio_tag)\n",
        "\n",
        "print(\"샘플 크기 : \\n\", len(sentences))\n",
        "print(\"0번째 샘플 단어 시퀀스 : \\n\", sentences[0])\n",
        "print(\"0번째 샘플 bio 태그 : \\n\", tags[0])\n",
        "print(\"샘플 단어 시퀀스 최대 길이 : \\n\", max(len(l) for l in sentences))\n",
        "print(\"샘플 단어 시퀀스 평균 길이 : \\n\", (sum(map(len, sentences))/len(sentences)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDhLIIB1RFIz"
      },
      "source": [
        "불러온 말뭉치 데이터에서 단어(w[1])와 BIO태그(w[3])만 불러와 학습용 데이터셋을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VE7uP526RHzI",
        "outputId": "25503b81-9eb8-4405-c402-475447ac14f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BIO 태그 사전 크기:  10\n",
            "단어 사전 크기:  17869\n"
          ]
        }
      ],
      "source": [
        "# 토크나이저 정의\n",
        "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그 정보는 lower=False: 소문자 변환 X\n",
        "tag_tokenizer.fit_on_texts(tags)\n",
        "\n",
        "# 단어 사전 및 태그 사전 크기\n",
        "vocab_size = len(p.word_index) + 1\n",
        "tag_size = len(tag_tokenizer.word_index) + 1\n",
        "print(\"BIO 태그 사전 크기: \", tag_size)\n",
        "print(\"단어 사전 크기: \", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_efhaRRIVg"
      },
      "source": [
        "단어 시퀀스는 미리 만들어 둔 Preprocess 객체에서 생성하므로 이 예제에서는 BIO 태그용 토크나이저 객체만 생성합니다.\n",
        "\n",
        "그리고 생성된 사전 리스트를 이용해 단어와 태그 사전의 크기를 정의해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNi-j5thRKvK"
      },
      "outputs": [],
      "source": [
        "# 학습용 단어 시퀀스 생성\n",
        "x_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
        "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
        "\n",
        "index_to_ner = tag_tokenizer.index_word # 시퀀스 인덱스를 NER로 변환\n",
        "index_to_ner[0] = 'PAD'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVADISHSRMcA"
      },
      "source": [
        "입력 문장은 Preprocess에서 생성한 단어 인덱스 시퀀스(get_wordidx_sequence)를 사용하여 번호 형태로 인코딩하고, BIO 태그는 위에서 만든 사전 데이터를 시퀀스 번호 형태로 인코딩합니다.\n",
        "\n",
        "그리고 모델 학습 후 모델이 예측한 문장의 태그 번호를 다시 NER로 변환해주기 위한 index_to_ner를 만들어줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFVSGjj0RNzg"
      },
      "outputs": [],
      "source": [
        "# 시퀀스 패딩 처리\n",
        "max_len = 40\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, padding='post', maxlen=max_len)\n",
        "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVuNt9QkRPbK"
      },
      "source": [
        "개체명 인식 모델의 입출력 베터 크기를 동일하게 맞추기 위해 단어 시퀀스의 평균 길이보다 넉넉하게 40으로 정의하여 시퀀스 패딩 작업을 실시합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPbaZBZeRP44",
        "outputId": "997fffdf-277e-4c64-9240-6f497bf99149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "학습 샘플 시퀀스 형상 :  (49599, 40)\n",
            "학습 샘플 레이블 형상 :  (49599, 40, 10)\n",
            "테스트 샘플 시퀀스 형상 :  (12400, 40)\n",
            "테스트 샘플 레이블 형상 :  (12400, 40, 10)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# 학습 데이터와 테스트 데이터 8:2 비율로 분리\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=.2, random_state=1234)\n",
        "\n",
        "# 출력 데이터 원-핫 인코딩\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
        "\n",
        "print(\"학습 샘플 시퀀스 형상 : \", x_train.shape)\n",
        "print(\"학습 샘플 레이블 형상 : \", y_train.shape)\n",
        "print(\"테스트 샘플 시퀀스 형상 : \", x_test.shape)\n",
        "print(\"테스트 샘플 레이블 형상 : \", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFSUlUP-RRKa"
      },
      "source": [
        "모델 학습을 위해 학습용과 테스트용 데이터셋을 8:2 비율로 분리합니다.\n",
        "\n",
        "이후 학습과 테스트용 출력 데이터(y_train, y_test)를 태그 사전 크기에 맞게 원-핫 인코딩을 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM9_4tFWRSwt",
        "outputId": "54a1b755-2ec8-4b2d-d504-231b8dd0160b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "388/388 [==============================] - 239s 594ms/step - loss: 0.0249 - accuracy: 0.9682\n",
            "Epoch 2/10\n",
            "388/388 [==============================] - 232s 598ms/step - loss: 0.0086 - accuracy: 0.9868\n",
            "Epoch 3/10\n",
            "388/388 [==============================] - 232s 598ms/step - loss: 0.0058 - accuracy: 0.9912\n",
            "Epoch 4/10\n",
            "388/388 [==============================] - 232s 597ms/step - loss: 0.0044 - accuracy: 0.9932\n",
            "Epoch 5/10\n",
            "388/388 [==============================] - 232s 597ms/step - loss: 0.0038 - accuracy: 0.9942\n",
            "Epoch 6/10\n",
            "388/388 [==============================] - 232s 598ms/step - loss: 0.0033 - accuracy: 0.9948\n",
            "Epoch 7/10\n",
            "388/388 [==============================] - 232s 598ms/step - loss: 0.0031 - accuracy: 0.9951\n",
            "Epoch 8/10\n",
            "388/388 [==============================] - 237s 611ms/step - loss: 0.0029 - accuracy: 0.9954\n",
            "Epoch 9/10\n",
            "388/388 [==============================] - 236s 608ms/step - loss: 0.0027 - accuracy: 0.9956\n",
            "Epoch 10/10\n",
            "388/388 [==============================] - 236s 608ms/step - loss: 0.0025 - accuracy: 0.9959\n",
            "388/388 [==============================] - 16s 41ms/step - loss: 0.0138 - accuracy: 0.9865\n",
            "평가 결과 :  0.9865468144416809\n"
          ]
        }
      ],
      "source": [
        "# 모델 정의(Bi-LSTM)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.50, recurrent_dropout=0.25)))\n",
        "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(0.01),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=10)\n",
        "\n",
        "print(\"평가 결과 : \", model.evaluate(x_test, y_test)[1])\n",
        "model.save('ner_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db7mAfjMUhG4"
      },
      "source": [
        "개체 인식 모델을 순차 모델 방식으로 구현한 코드입니다. tag_size 만큼의 출력 뉴런에서 제일 확률이 높은 출력값 1개를 선택하는 문제이므로 모델 출력 계층의 활성화 함수로 softmax를 사용하였고, 손실 함수는 categorical_crossentropy를 사용했습니다.\n",
        "\n",
        "마지막으로 학습이 완료된 모델을 이후 챗봇 엔진의 개체명 인식 모듈에서 사용할 수 있도록 h5 파일 포맷으로 저장해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DmWb-Ji6UFK8"
      },
      "outputs": [],
      "source": [
        "# F1 스코어 계산을 위해 사용\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# 시퀀스를 NER 태그로 변환\n",
        "def sequences_to_tag(sequences): # 예측값을 index_to_ner을 사용하여 태깅 정보로 변환\n",
        "    result = []\n",
        "    for sequence in sequences: # 전체 시퀀스로부터 시퀀스를 하나씩 꺼낸다\n",
        "        temp = []\n",
        "        for pred in sequence: # 시퀀스로부터 예측값을 하나씩 꺼낸다\n",
        "            pred_index = np.argmax(pred) # ex) [0, 0, 1, 0, 0]이라면 1의 인덱스인 2를 리턴\n",
        "            temp.append(index_to_ner[pred_index].replace(\"PAD\", \"0\")) # 'PAD'는 '0'으로 변경\n",
        "        result.append(temp)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUSnL-q2Uk2M"
      },
      "source": [
        "학습이 완료된 모델을 통해 테스트용 데이터셋의 예측 결과(시퀀스 번호 형태)를 다시 NER 태그로 변환해주기 위한 함수를 만들어줍니다. 해당 함수의 입력은 시퀀스 번호로 인코딩된 테스트용 단어 시퀀스(넘파이 배열)을 사용하고, 해당 함수의 결과로는 예측된 NER 태그 정보가 담긴 넘파일 배열이 반환됩니다.\n",
        "\n",
        "NER 태그별로 계산된 정밀도(precision)와 재현율(recall), F1 스코어를 출력하는 `seqeval.metrics` 모듈의 classification_report() 함수를 이용합니다. (f1_score() 함수를 통해 F1 스코어값만 불러올 수도 있습니다.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJxbZEBkUibh"
      },
      "outputs": [],
      "source": [
        "# 테스트 데이터셋의 NER 예측\n",
        "y_predicted = model.predict(x_test)\n",
        "pred_tags = sequences_to_tag(y_predicted) # 예측된 NER\n",
        "test_tags = sequences_to_tag(y_test) # 실제 NER\n",
        "\n",
        "# F1 평가 결과\n",
        "print(classification_report(test_tags, pred_tags))\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(test_tags, pred_tags)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJGW1chAUmgM"
      },
      "source": [
        "본 모델에서는 예측 결과의 평균 F1 스코어가 96.9%로 나왔고, 이는 이전 3-1편에서 사용한 모델에 비해 더 높은 F1 스코어가 나온 것을 확인할 수 있습니다. \n",
        "\n",
        "### 4.2 개체명 인식 모듈 생성\n",
        "\n",
        "이제 앞서 학습한 개체명 인식 모델 파일을 활용해 입력 문장 내부의 개체명을 인식하는 챗봇 엔진의 개체명 인식 모듈을 만들어 볼 차례입니다. 해당 모듈은 딥러닝 모델이기때문에 /models/ner 디렉터리 내에 NerModel.py 소스 파일을 생성하겠습니다.\n",
        "\n",
        "- **챗봇 엔진 NER 모델 모듈**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1iPbFGz9Un6O"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing\n",
        "\n",
        "# 개체명 인식 모델 모듈\n",
        "class NerModel:\n",
        "    def __init__(self, model_name, proprocess):\n",
        "\n",
        "        # BIO 태그 클래스별 레이블\n",
        "        self.index_to_ner = {1:'0', 2:'B_DT', 3:'B_FOOD', 4:'I', 5:'B_OG',\n",
        "                             6:'B_PS', 7:'B_LC', 8:'NNP', 9:'B_TI', 0:'PAD'}\n",
        "\n",
        "        # 의도 분류 모델 불러오기\n",
        "        self.model = load_model(model_name)\n",
        "\n",
        "        # 챗봇 Preprocess 객체\n",
        "        self.p = proprocess\n",
        "\n",
        "    # 개체명 클래스 예측\n",
        "    def predict(self, query):\n",
        "        # 형태소 분석\n",
        "        pos = self.p.pos(query)\n",
        "\n",
        "        # 문장 내 키워드 추출(불용어 제거)\n",
        "        keywords = self.p.get_keywors(pos, without_tag=True)\n",
        "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
        "\n",
        "        # 패딩처리\n",
        "        max_len = 40\n",
        "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding='post', value=0, maxlen=max_len)\n",
        "\n",
        "        # 키워드별 개체명 예측\n",
        "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
        "        predict_class = tf.math.argmax(predict, axis=-1)\n",
        "\n",
        "        tags = [self.index_to_ner[i] for i in predict_class.numpy()[0]]\n",
        "        return list(zip(keywords, tags))\n",
        "\n",
        "    def predict_tags(self, query):\n",
        "        # 형태소 분석\n",
        "        pos = self.p.pos(query)\n",
        "\n",
        "        # 문장 내 키워드 추출(불용어 제거)\n",
        "        keywords = self.p.get_keywors(pos, without_tag=True)\n",
        "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
        "\n",
        "        # 패딩처리\n",
        "        max_len = 40\n",
        "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding='post', value=0, maxlen=max_len)\n",
        "\n",
        "        # 키워드별 개체명 예측\n",
        "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
        "        predict_class = tf.math.argmax(predict, axis=-1)\n",
        "\n",
        "        tags = []\n",
        "        for tag_idx in predict_class.numpy()[0]:\n",
        "            if tag_idx == 1: continue\n",
        "            tags.append(self.index_to_ner[tag_idx])\n",
        "            \n",
        "        if len(tags) == 0:\n",
        "            return None\n",
        "        return tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSfGfGsrUrjq"
      },
      "source": [
        "앞서 만든 NerModel 클래스를 테스트하는 코드입니다. NerModel 객체를 생성해 새로운 유형의 문장에서 개체명을 인식합니다.\n",
        "\n",
        "테스트 코드이므로 /test 디렉터리에 model_ner_test.py 파일을 생성해주세요.\n",
        "\n",
        "- **NerModel 객체 사용**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OSV8itHvUpMd",
        "outputId": "a9e1b740-708c-4c13-abb3-6c0f0c727ead"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('내일', 'B_DT'), ('오후', 'B_DT'), ('2시', 'B_DT'), ('30분', 'B_DT'), ('짜장면', 'B_FOOD'), ('주문', 'O'), ('싶', 'O')]\n"
          ]
        }
      ],
      "source": [
        "from utils.Preprocess import Preprocess\n",
        "from models.ner.NerModel import NerModel\n",
        "\n",
        "p = Preprocess(word2index_dic='./train_tools/dict/chatbot_dict.bin',\n",
        "                  userdic = './utils/user_dic.tsv')\n",
        "\n",
        "ner = NerModel(model_name='./models/ner/ner_model.h5', proprocess=p)\n",
        "query = '내일 오후 2시 30분에 짜장면 주문하고 싶어요'\n",
        "predicts = ner.predict(query)\n",
        "print(predicts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTPUsNIgUtqR"
      },
      "source": [
        "결과는 다음과 같습니다. 테스트 예문이 학습 데이터 유형과 비슷해 학습한 모델이 개체명들을 잘 인식한 결과를 볼 수 있습니다. 더 다양한 유형의 문장을 학습하면 NER 품질이 더 좋아집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "737iY2j8Uv2Z"
      },
      "source": [
        "## 5. 답변 검색\n",
        "\n",
        "챗봇의 답변 검색은 그 자체만으로도 방대한 양의 지식을 필요로 하는 분야이지만 이 게시글에서는 단순한 SQL구문을 사용해 **룰 베이스 기반**으로 답변을 검색하는 방법을 소개하겠습니다. \n",
        "\n",
        "### 5.1 데이터베이스 제어 모듈 생성\n",
        "\n",
        "/utils 디렉터리 내에 Database.py 파일을 생성해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7SOmlQUUxzR"
      },
      "outputs": [],
      "source": [
        "import pymysql\n",
        "import pymysql.cursors\n",
        "import logging\n",
        "\n",
        "\n",
        "class Database:\n",
        "    '''\n",
        "    데이터베이스 제어\n",
        "    '''\n",
        "\n",
        "    def __init__(self, host, user, password, db_name, charset='utf8'):\n",
        "        self.host = host\n",
        "        self.user = user\n",
        "        self.password = password\n",
        "        self.charset = charset\n",
        "        self.db_name = db_name\n",
        "        self.conn = None\n",
        "\n",
        "    # DB 연결\n",
        "    def connect(self):\n",
        "        if self.conn is not None:\n",
        "            return\n",
        "\n",
        "        self.conn = pymysql.connect(\n",
        "            host=self.host,\n",
        "            user=self.user,\n",
        "            password=self.password,\n",
        "            db=self.db_name,\n",
        "            charset=self.charset\n",
        "        )\n",
        "\n",
        "    # DB 연결 닫기\n",
        "    def close(self):\n",
        "        if self.conn is None:\n",
        "            return\n",
        "\n",
        "        if not self.conn.open:\n",
        "            self.conn = None\n",
        "            return\n",
        "        self.conn.close()\n",
        "        self.conn = None\n",
        "\n",
        "    # SQL 구문 실행\n",
        "    def execute(self, sql):\n",
        "        last_row_id = -1\n",
        "        try:\n",
        "            with self.conn.cursor() as cursor:\n",
        "                cursor.execute(sql)\n",
        "            self.conn.commit()\n",
        "            last_row_id = cursor.lastrowid\n",
        "            # logging.debug(\"execute last_row_id : %d\", last_row_id)\n",
        "        except Exception as ex:\n",
        "            logging.error(ex)\n",
        "\n",
        "        finally:\n",
        "            return last_row_id\n",
        "\n",
        "    # SELECT 구문 실행 후 단 1개의 데이터 ROW만 불러옴\n",
        "    def select_one(self, sql):\n",
        "        result = None\n",
        "\n",
        "        try:\n",
        "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
        "                cursor.execute(sql)\n",
        "                result = cursor.fetchone()\n",
        "        except Exception as ex:\n",
        "            logging.error(ex)\n",
        "\n",
        "        finally:\n",
        "            return result\n",
        "\n",
        "    # SELECT 구문 실행 후 전체 데이터 ROW를 불러옴\n",
        "    def select_all(self, sql):\n",
        "        result = None\n",
        "\n",
        "        try:\n",
        "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
        "                cursor.execute(sql)\n",
        "                result = cursor.fetchall()\n",
        "        except Exception as ex:\n",
        "            logging.error(ex)\n",
        "\n",
        "        finally:\n",
        "            return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOMsapxYUzZ_"
      },
      "source": [
        "### 5.2 답변 검색 모듈 생성\n",
        "\n",
        "![답변 검색 모듈 생성 (1).png](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/cc60457a-befb-4b91-a601-7ff9b4e81fec/%E1%84%83%E1%85%A1%E1%86%B8%E1%84%87%E1%85%A7%E1%86%AB_%E1%84%80%E1%85%A5%E1%86%B7%E1%84%89%E1%85%A2%E1%86%A8_%E1%84%86%E1%85%A9%E1%84%83%E1%85%B2%E1%86%AF_%E1%84%89%E1%85%A2%E1%86%BC%E1%84%89%E1%85%A5%E1%86%BC_%281%29.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220809%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220809T015849Z&X-Amz-Expires=86400&X-Amz-Signature=43a0e808696dbd8b94109708f9460c7f65cea2c5126202e409a1886b946fd518&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22%25E1%2584%2583%25E1%2585%25A1%25E1%2586%25B8%25E1%2584%2587%25E1%2585%25A7%25E1%2586%25AB%2520%25E1%2584%2580%25E1%2585%25A5%25E1%2586%25B7%25E1%2584%2589%25E1%2585%25A2%25E1%2586%25A8%2520%25E1%2584%2586%25E1%2585%25A9%25E1%2584%2583%25E1%2585%25B2%25E1%2586%25AF%2520%25E1%2584%2589%25E1%2585%25A2%25E1%2586%25BC%25E1%2584%2589%25E1%2585%25A5%25E1%2586%25BC%2520%281%29.png%22&x-id=GetObject)\n",
        "\n",
        "챗봇/utils 디렉터리에 FindAnswer.py 파일에 코드를 작성해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkDwEJZZU5V-"
      },
      "outputs": [],
      "source": [
        "class FindAnswer:\n",
        "    def __init__(self, db):\n",
        "        self.db = db\n",
        "\n",
        "    # 검색 쿼리 생성\n",
        "    def _make_query(self, intent_name, ner_tags):\n",
        "        sql = \"select * from chatbot_train_data\"\n",
        "        if intent_name is not None and ner_tags is None:\n",
        "            sql = sql + \" where intent='{}' \".format(intent_name)\n",
        "\n",
        "        elif intent_name is not None and ner_tags is not None:\n",
        "            where = \" where intent='%s' \" % intent_name\n",
        "            if len(ner_tags) > 0:\n",
        "                where += \"and (\"\n",
        "                for ne in ner_tags:\n",
        "                    where += \" ner like '%{}%' or \".format(ne)\n",
        "                where = where[:-3] + ')'\n",
        "            sql = sql + where\n",
        "\n",
        "        # 동일한 답변이 2개 이상인 경우 랜덤으로 선택\n",
        "        sql = sql + \" order by rand() limit 1\"\n",
        "        return sql\n",
        "\n",
        "    # 답변 검색\n",
        "    def search(self, intent_name, ner_tags):\n",
        "        # 의도명과 개체명으로 답변 검색\n",
        "        sql = self._make_query(intent_name, ner_tags)\n",
        "        answer = self.db.select_one(sql)\n",
        "\n",
        "        # 검색되는 답변이 없으면 의도명만 검색\n",
        "        if answer is None:\n",
        "            sql = self._make_query(intent_name, None)\n",
        "            answer = self.db.select_one(sql)\n",
        "\n",
        "        return answer['answer'], answer['answer_image']\n",
        "\n",
        "    # NER 태그를 실제 입력된 단어로 변환\n",
        "    def tag_to_word(self, ner_predicts, answer):\n",
        "        for word, tag in ner_predicts:\n",
        "\n",
        "            # 변환해야 하는 태그가 있는 경우 추가\n",
        "            if tag == 'B_FOOD':\n",
        "                answer = answer.replace(tag, word)\n",
        "\n",
        "        answer = answer.replace('{', '')\n",
        "        answer = answer.replace('}', '')\n",
        "        return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6iJXYLeU88r"
      },
      "source": [
        "## 6. 챗봇 엔진 서버 개발\n",
        "\n",
        "다양한 플랫폼에서 언제든지 사용할 수 있도록 서버용 프로그램을 만들어야합니다. 이 절에서는 서버 통신 기능을 구현하겠습니다. \n",
        "\n",
        "### 6.1 통신 프로토콜 정의\n",
        "\n",
        "챗봇 엔진 서버와 통신하기 위해서는 프로토콜이 필요합니다. 프로토콜이란 서버와 클라이언트 간의 통신을 위한 규약입니다. Key/Value 쌍으로 이루어진 데이터 객체를 전달하는 JSON 형태를 주로 사용합니다. 챗봇 엔진은 양방향 통신이기 때문에 두 가지 형태의 프로토콜을 정의해야합니다. \n",
        "\n",
        "먼저, 클라이언트에서 서버 쪽으로 요청하는 JSON 프로토콜 예시입니다. \n",
        "\n",
        "Query는 챗봇 엔진에 요청하는 질의 텍스트이고 BotType은 서버에 접속하는 챗봇 서비스 타입입니다.\n",
        "\n",
        "```json\n",
        "{\n",
        "\t\"Query\" : \"자장면 주문할게요\",\n",
        "\t\"BotType\" : \"Kakao\"\n",
        "}\n",
        "```\n",
        "\n",
        "다음은 챗봇 엔진의 처리 결과를 클라이언트 쪽에 응답하는 JSON 프로토콜 예시입니다. \n",
        "\n",
        "Query는 챗봇 엔진에 요청한 질의 텍스트이며, Intent는 챗봇 엔진이 해석한 질의 텍스트의 의도, NER은 인ㄴ식된 개체명, Answer는 요청한 질의의 답변 텍스트입니다. 답변에 이미지가 존재하는 경우 AnswerInmageUrl에 이미지 경로가 있습니다. \n",
        "\n",
        "```json\n",
        "{\n",
        "\t\"Query\" : \"자장면 주문할게요\",\n",
        "\t\"Intent\" : \"주문\",\n",
        "\t\"NER\" : \"[('자장면', 'B_FOOD'), ('주문', 'O')]\",\n",
        "\t\"Answer\" : \"자장면 주문 처리 감사\",\n",
        "\t\"AnswerImageUrl\" : \"\"\n",
        "}\n",
        "```\n",
        "\n",
        "아래 그림은 서버와 클라이언트 간에 JSON 데이터를 주고받는 과정입니다.\n",
        "\n",
        "![Untitled](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/331d0ee2-57b1-4784-b89d-b76f05371593/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20220809%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20220809T020025Z&X-Amz-Expires=86400&X-Amz-Signature=7bc6f4147f3c3b54a326ec2aa1b98368efcf8f1e69bdbe5302c602fa09fb39e0&X-Amz-SignedHeaders=host&response-content-disposition=filename%20%3D%22Untitled.png%22&x-id=GetObject)\n",
        "\n",
        "### 6.2 다중 접속을 위한 TCP 소켓 서버\n",
        "\n",
        "지금가지는 한 번에 하나의 작업만 실행하는 **싱글 스레드(single thread)** 방식이었습니다. 다른 클라이언트가 서비스를 받고 있는 경우에는 챗봇 엔진의 응답을 받지 못하는 문제가 있습니다. \n",
        "\n",
        "그래서 **멀티 스레드(multi thread)** 방식을 사용합니다. **멀티 스레드**란 하나의 프로그램이 동시에 여러 개의 작업을 할 수 있도록 하는 방법입니다. 여기서 스레드란 프로그램 내에서 실행되는 단위입니다. 어떤 함수를 하나 호출하면 하나의 스레드가 생겨서 실행됩니다. \n",
        "\n",
        "챗봇 엔진 서버는 챗봇 클라이언트가 연결 요청을 할 때마다 챗봇 엔진 처리 스레드를 생성해 일련의 과정을 거처 요청한 질의의 답변을 클라이언트 쪽으로 전송합니다. 챗봇 클라이언트의 요청이 동시에 들어온다면 챗봇 엔진 처리 스레드는 클라이언트 연결 요청 수만큼 생성되어 동작합니다. \n",
        "\n",
        "TCP 소켓 서버를 관리하는 모듈을 먼저 만들어봅시다. 이 모듈은 서버에 접속하는 클라이언트 소켓을 생성하고 처리하는 기능을 합니다. /utils 디렉터리 내에 [BotServer.py](http://BotServer.py) 파일을 생성해주세요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVYRr7-0VSpe"
      },
      "outputs": [],
      "source": [
        "import socket\n",
        "\n",
        "class BotServer:\n",
        "    def __init__(self, srv_port, listen_num):\n",
        "        self.port = srv_port\n",
        "        self.listen = listen_num\n",
        "        self.mySock = None\n",
        "\n",
        "    # sock 생성\n",
        "    def create_sock(self):\n",
        "        self.mySock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "        self.mySock.bind((\"0.0.0.0\", int(self.port)))\n",
        "        self.mySock.listen(int(self.listen))\n",
        "        return self.mySock\n",
        "\n",
        "    # client 대기\n",
        "    def ready_for_client(self):\n",
        "        return self.mySock.accept()\n",
        "\n",
        "    # sock 반환\n",
        "    def get_sock(self):\n",
        "        return self.mySo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ5293bSVTHs"
      },
      "source": [
        "다음은 챗봇 엔진 서버 프로그램을 완성하겠습니다. BotServer 클래스와 멀티 스레드 모듈을 이용합니다. 다음 코드는 챗봇 엔진 메인 프로그램이기 때문에 프로젝트 루트 디렉터리에 bot.py 파일로 생성합니다. \n",
        "\n",
        "```python\n",
        "import threading\n",
        "import json\n",
        "\n",
        "from config.DatabaseConfig import *\n",
        "from utils.Database import Database\n",
        "from utils.BotServer import BotServer\n",
        "from utils.Preprocess import Preprocess\n",
        "from models.intent.IntentModel import IntentModel\n",
        "from models.ner.NerModel import NerModel\n",
        "from utils.FindAnswer import FindAnswer\n",
        "\n",
        "\n",
        "# 전처리 객체 생성\n",
        "p = Preprocess(word2index_dic='train_tools/dict/chatbot_dict.bin',\n",
        "               userdic='utils/user_dic.tsv')\n",
        "\n",
        "# 의도 파악 모델\n",
        "intent = IntentModel(model_name='models/intent/intent_model.h5', proprocess=p)\n",
        "\n",
        "# 개체명 인식 모델\n",
        "ner = NerModel(model_name='models/ner/ner_model.h5', proprocess=p)\n",
        "\n",
        "\n",
        "def to_client(conn, addr, params):\n",
        "    db = params['db']\n",
        "\n",
        "    try:\n",
        "        db.connect()  # 디비 연결\n",
        "\n",
        "        # 데이터 수신\n",
        "        read = conn.recv(2048)  # 수신 데이터가 있을 때 까지 블로킹\n",
        "        print('===========================')\n",
        "        print('Connection from: %s' % str(addr))\n",
        "\n",
        "        if read is None or not read:\n",
        "            # 클라이언트 연결이 끊어지거나, 오류가 있는 경우\n",
        "            print('클라이언트 연결 끊어짐')\n",
        "            exit(0)\n",
        "\n",
        "\n",
        "        # json 데이터로 변환\n",
        "        recv_json_data = json.loads(read.decode())\n",
        "        print(\"데이터 수신 : \", recv_json_data)\n",
        "        query = recv_json_data['Query']\n",
        "\n",
        "        # 의도 파악\n",
        "        intent_predict = intent.predict_class(query)\n",
        "        intent_name = intent.labels[intent_predict]\n",
        "\n",
        "        # 개체명 파악\n",
        "        ner_predicts = ner.predict(query)\n",
        "        ner_tags = ner.predict_tags(query)\n",
        "\n",
        "\n",
        "        # 답변 검색\n",
        "        try:\n",
        "            f = FindAnswer(db)\n",
        "            answer_text, answer_image = f.search(intent_name, ner_tags)\n",
        "            answer = f.tag_to_word(ner_predicts, answer_text)\n",
        "\n",
        "        except:\n",
        "            answer = \"죄송해요 무슨 말인지 모르겠어요. 조금 더 공부 할게요.\"\n",
        "            answer_image = None\n",
        "\n",
        "        send_json_data_str = {\n",
        "            \"Query\" : query,\n",
        "            \"Answer\": answer,\n",
        "            \"AnswerImageUrl\" : answer_image,\n",
        "            \"Intent\": intent_name,\n",
        "            \"NER\": str(ner_predicts)\n",
        "        }\n",
        "        message = json.dumps(send_json_data_str)\n",
        "        conn.send(message.encode())\n",
        "\n",
        "    except Exception as ex:\n",
        "        print(ex)\n",
        "\n",
        "    finally:\n",
        "        if db is not None: # db 연결 끊기\n",
        "            db.close()\n",
        "        conn.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # 질문/답변 학습 디비 연결 객체 생성\n",
        "    db = Database(\n",
        "        host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME\n",
        "    )\n",
        "    print(\"DB 접속\")\n",
        "\n",
        "    port = 5050\n",
        "    listen = 100\n",
        "\n",
        "    # 봇 서버 동작\n",
        "    bot = BotServer(port, listen)\n",
        "    bot.create_sock()\n",
        "    print(\"bot start\")\n",
        "\n",
        "    while True:\n",
        "        conn, addr = bot.ready_for_client()\n",
        "        params = {\n",
        "            \"db\": db\n",
        "        }\n",
        "\n",
        "        client = threading.Thread(target=to_client, args=(\n",
        "            conn,\n",
        "            addr,\n",
        "            params\n",
        "        ))\n",
        "        client.start()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LovRXLrIVXq2"
      },
      "source": [
        "드디어 챗봇 엔진 서버가 완성되었습니다. 해당 프로그램이 서버 환경에서 백그라운드 프로그램으로 동작한다면 24시간 챗봇 서비스를 운영할 수 있습니다. \n",
        "\n",
        "마지막으로 챗봇 엔진 서버 프로그램에 접속하는 클라이언트 프로그램을 만들어서 챗봇 엔진 동작을 테스트해봅시다. \n",
        "\n",
        "### 6.3 챗봇 테스트 클라이언트 프로그램\n",
        "\n",
        "챗봇 엔진 서버를 테스트할 수 있는 클라이언트 프로그램을 간단하게 만들어보겠습니다. 챗봇 테스트 클라이언트 프로그램은 콘솔 화면에 질문을 입력하면 챗봇 엔진 서버와 통신해 결과를 출력합니다. /test 디렉터리에 chatbot_client_test.py 파일을 생성합니다.\n",
        "\n",
        "```python\n",
        "import socket\n",
        "import json\n",
        "\n",
        "# 챗봇 엔진 서버 접속 정보\n",
        "host = \"127.0.0.1\"  # 챗봇 엔진 서버 IP 주소\n",
        "port = 5050  # 챗봇 엔진 서버 통신 포트\n",
        "\n",
        "# 클라이언트 프로그램 시작\n",
        "while True:\n",
        "    print(\"질문 : \")\n",
        "    query = input()  # 질문 입력\n",
        "    if(query == \"exit\"):\n",
        "        exit(0)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # 챗봇 엔진 서버 연결\n",
        "    mySocket = socket.socket()\n",
        "    mySocket.connect((host, port))\n",
        "\n",
        "    # 챗봇 엔진 질의 요청\n",
        "    json_data = {\n",
        "        'Query': query,\n",
        "        'BotType': \"MyService\"\n",
        "    }\n",
        "    message = json.dumps(json_data)\n",
        "    mySocket.send(message.encode())\n",
        "\n",
        "    # 챗봇 엔진 답변 출력\n",
        "    data = mySocket.recv(2048).decode()\n",
        "    ret_data = json.loads(data)\n",
        "    print(\"답변 : \")\n",
        "    print(ret_data['Answer'])\n",
        "    print(ret_data)\n",
        "    print(type(ret_data))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # 챗봇 엔진 서버 연결 소켓 닫기\n",
        "    mySocket.close()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpN3T24gVbiv"
      },
      "source": [
        "우리가 학습한 데이터를 토대로 적절한 답변을 출력하고 있습니다. 더 많은 데이터로 다양한 의도와 개체명, 질문에 맞는 답변을 보강한다면 멋진 챗봇을 만들 수 있을것입니다! \n",
        "\n",
        "## 마무리\n",
        "\n",
        "이번 시간에는 딥러닝 모델을 적용하여 간단한 챗봇 엔진 서버 프로그램을 구현해보았습니다!\n",
        "\n",
        "다음 시간에는 이번에 만든 챗봇 엔진을 외부 메신저 플랫폼과 연동하기 위해 필요한 내용을 알아보도록 하겠습니다!\n",
        "\n",
        "감사합니다 😊\n",
        "\n",
        "📍Irene팀의 모든 포스팅은 <*처음 배우는 딥러닝 챗봇*> 서적을 기반으로 합니다"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}