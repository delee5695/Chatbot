{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **토크나이징**\n",
        "\n",
        "## 1.1 토크나이징 소개\n",
        "\n",
        "**자연어 처리(Natural Language Processing ; NLP) 란?**\n",
        "\n",
        "컴퓨터는 자연어를 직접적으로 이해할 수 없기 때문에, 자연어의 의미를 분석해 컴퓨터가 처리할 수 있도록 하는 일\n",
        "\n",
        "**토큰(Token)**\n",
        "\n",
        "- 토큰 : 문장을 일정한 의미가 있는 가장 작은 정보 단위\n",
        "- 토크나이징 : 문장 형태의 데이터를 토큰 단위로 나누는 것\n",
        "\n",
        "텍스트 전처리 과정에서 사용되는 토크나이징은 문장 형태의 데이터를 처리하기 위해 제일 처음 수행해야하는 기본적이고 중요한 작업입니다. \n",
        "\n",
        "토크나이징은 언어의 특성과 문법에 따라 다양한 방법이 필요합니다. 한글과 영어는 구성 요소가 다르기 때문에 같은 토크나이징 방법을 사용한다면 곤란해집니다. 한글은 조사를 제외하고, 영어는 apostrophe를 원래 의미로 전환해주는 등의 작업이 필요합니다. 또한, 토크나이징 방법에 따라 모델 성능 차이가 날 수 있습니다. \n",
        "\n",
        "지금부터 토크나이징 방법에 대해 알아보겠습니다."
      ],
      "metadata": {
        "id": "0Zyaj5wUG4W4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2  KoNLPy\n",
        "\n",
        "**KoNLPy(코엔엘파이)**는 기본적인 한국어 자연어 처리를 위한 파이썬 라이브러리입니다. 오픈소스 소프트웨어이며 GPL v3 라이선스에 따라 자유롭게 코드를 사용할 수 있다는 장점이 있습니다. \n",
        "\n",
        "KoNLPy는 **형태소 분석기**를 제공합니다. 한국어는 명사와 조사를 띄어쓰지 않고, 용언에 따라 여러가지 어미가 붙기 때문에 띄어쓰기, 공백 기준의 토크나이징은 부족합니다. 따라서 문장에서 형태소를 분석하는 도구가 필요합니다. 형태소 분석기는 문장에서 **형태소뿐만 아니라 어근, 접두사/접미사, 품사 등 다양한 언어적 속성의 구조**를 파악해줍니다. \n",
        "\n",
        "한국어의 복잡한 문법적인 구조 때문에 완벽한 형태소 분석기를 개발하는 일은 쉽지 않습니다. 다행히 KoNLPy는 실무에서도 사용하기 좋은 몇 가지 형태소 분석기를 통합해 라이브러리로 제공합니다. \n",
        "\n",
        "지금부터 KoNLPy에서 제공하는 세 가지 종류의 형태소 분석기를 알아보겠습니다."
      ],
      "metadata": {
        "id": "Fm8edx414nJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1.2.1 Kkma\n",
        "\n",
        "**꼬꼬마(kkma) 형태소** \n",
        "\n",
        "- 서울대학교 IDS 연구실에서 개발한 한국어 형태소 분석기\n",
        "- 꼬꼬마 형태소 분석기를 사용하는 방법\n",
        "    \n",
        "  "
      ],
      "metadata": {
        "id": "zDPoSiT44tCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install konlpy\n",
        "from konlpy.tag import Kkma"
      ],
      "metadata": {
        "id": "lT0d6IMU_V92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c453851c-2078-480a-90d7-a3acd168b2a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (453 kB)\n",
            "\u001b[K     |████████████████████████████████| 453 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kkma 모듈의 함수 설명**\n",
        "\n",
        "- morphs(phrase) : 인자로 입력한 문장을 형태소 단위로 토크나이징 합니다. 토크나이징된 형태소들은 리스트 형태로 반환됩니다.\n",
        "- nouns(phrase) : 인자로 입력한 문장에서 품사가 명사인 토큰만 추출합니다.\n",
        "- pos(phrase, flatten=True) : POS tagger라 부르며, 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅을 합니다. 추출된 형태소와 그 형태소의 품사가 튜플 형태로 묶여서 리스트로 반환됩니다.\n",
        "- sentences(phrase) : 인자로 입력한 여러 문장을 분리해주는 역할을 합니다. 분리된 문장은 리스트 형태로 반환됩니다.\n"
      ],
      "metadata": {
        "id": "fCP34qOc4_0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "\n",
        "# 꼬꼬마 형태소 분석기 객체 생성\n",
        "kkma = Kkma()\n",
        "\n",
        "text = \"아버지가 방에 들어갑니다.\"\n",
        "\n",
        "# 형태소 추출\n",
        "morphs = kkma.morphs(text) # text 변수에 저장된 문장을 형태소 단위로 토크나이징 합니다. \n",
        "print(morphs) \n",
        "\n",
        "# 형태소와 품사 태그 추출\n",
        "pos = kkma.pos(text) # text 변수에 저장된 문장을 품사 태깅 합니다. \n",
        "print(pos) \n",
        "\n",
        "# 명사만 추출\n",
        "nouns = kkma.nouns(text) # text 변수에 저장된 문장에서 품사가 명사인 단어들만 추출합니다. \n",
        "print(nouns) \n",
        "\n",
        "# 문장 분리\n",
        "sentences = \"오늘 날씨는 어때요? 내일은 덥다던데.\" \n",
        "s = kkma.sentences(sentences) # 복합 문장이 있을 대 문장 단위로 토크나이징 합니다. \n",
        "print(s) "
      ],
      "metadata": {
        "id": "LXWZYksyAOfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cae0275-2ca8-4d32-b923-a3754484f996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['아버지', '가', '방', '에', '들어가', 'ㅂ니다', '.']\n",
            "[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKM'), ('들어가', 'VV'), ('ㅂ니다', 'EFN'), ('.', 'SF')]\n",
            "['아버지', '방']\n",
            "['오늘 날씨는 어 때요?', '내일은 덥다 던데.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Kkma 품사 태그표**\n",
        "\n",
        "형태소 분석기 출력 결과를 비교했을 때, 문장에 따라 정확한 형태소 분석이 안될 수 도 있습니다. \n",
        "\n",
        "[**Kkma documents**](http://kkma.snu.ac.kr/)"
      ],
      "metadata": {
        "id": "jOjE66SZ5EAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Komoran \n",
        "\n",
        "**코모란 형태소**\n",
        "\n",
        "- Shineware에서 자바로 개발한 한국어 형태소 분석기\n",
        "- 다른 형태소 분석기와 다르게 고백이 포함된 형태소 단위로도 분석이 가능합니다.\n",
        "- 불러오는 방법 : from konlpy.tag import Komoran\n",
        "\n",
        "**Komoran 모듈의 함수 설명**\n",
        "\n",
        "- morphs(phrase) : 인자로 입력한 문장을 형태소 단위로 토크나이징합니다. 토크나이징된 형태소들은 리스트 형태로 반환됩니다.\n",
        "- noun(phrase) : 인자로 입력한 문장에서 품사가 명사인 토큰들만 추출합니다.\n",
        "- pos(phrase, flatten=True): POS tagger라 부르며, 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅을 합니다. 추출된 형태소와 그 형태소의 품사가 튜플 형태로 묶여서 리스트로 반환됩니다."
      ],
      "metadata": {
        "id": "nrWU8JF85T_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 품사 태그표\n",
        "\n",
        "| 체언 | 명사NN | 일반명사NNG<br/>고유명사NNP<br/>의존명사NNB |\n",
        "| --- | --- | --- |\n",
        "|  | 대명사NP | 대명사NP<br/> 수사NR |\n",
        "| 용언 | 동사VV | 동사VV |\n",
        "|  | 형용사VA | 형용사VA |\n",
        "|  | 보조용언VX | 보조용언VX |\n",
        "|  | 지정사VC | 긍정지정사VCP<br/>부정지정사VCN |\n",
        "| 수식언 | 관형사MM | 관형사MM |\n",
        "|  | 부사MA | 일반부사MAG<br/>접속부사MAJ |\n",
        "| 독립언 | 감탄사IC | 감탄사IC |\n",
        "| 관계언 | 격조사JK | 주격조사JKS<br/>보격조사JKC<br/>관형격조사JKG<br/>목적격조사JKO<br/>부사격조사JKB<br/>호격조사JKV<br/>인용격조사JKQ |\n",
        "|  | 보조사JX | 보조사JX |\n",
        "|  | 접속조사JC | 접속조사JC |\n",
        "| 의존형태 | 어미E | 선어말어미EP<br/>종결어미EF<br/>연결어미EC<br/>명사형전성어미ETN<br/>관형형전성어미ETM |\n",
        "|  | 접두사XP | 체언접두사XPN |\n",
        "|  | 접미사XS | 명사파생접미사XSN<br/>동사파생접미사XSV<br/>형용사파생접미사XSA |\n",
        "|  | 어근XR | 어근XR |\n",
        "| 기호 |  | 마침표,물음표,느낌표SF<br/> 쉼표,가운뎃점,콜론,빗금SP<br/> 따옴표,괄호표,줄표SS<br/> 줄임표SE<br/>  붙임표(물결,숨김,빠짐)SO<br/> 외국어SL<br/> 한자SH<br/> 기타기호(논리수학기호,화폐기호)SW<br/> 명사추정범주NF<br/> 용언추정범주NV<br/> 숫자SN<br/> 분석불능범주NA |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4LQGRTCY5YV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**형태소 추출**\n",
        "\n",
        "text 변수에 저장된 문장을 형태소 단위로 토크나이징합니다. 토크나이징된 형태소들은 리스트 형태로 반환됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "CCTp1vaP65Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "# 코모란 형태소 분석기 객체 생성\n",
        "komoran=Komoran()\n",
        "text=\"우리 함께 열심히 공부합시다\"\n",
        "morphs=komoran.morphs(text)\n",
        "\n",
        "print(morphs)"
      ],
      "metadata": {
        "id": "-F912UhGAVce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4848a6-9ab7-47e7-93d3-c9187a85ceb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['우리', '함께', '열심히', '공부', '하', 'ㅂ시다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**형태소와 품사 태그 추출**\n",
        "\n",
        "text 변수에 저장된 문장을 품사 태깅합니다. 태깅된 품사와 형태소들은 리스트 형태로 반환됩니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "0hcUUvnS7COh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#형태소와 품사 태그 추출\n",
        "pos=komoran.pos(text)\n",
        "print(pos)\n",
        "\n",
        "#명사만 추출\n",
        "noun=komoran.nouns(text)\n",
        "print(noun)"
      ],
      "metadata": {
        "id": "TxvfrMPGAY8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a54729f-6a09-4020-dfd1-c0b33bdb801e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('우리', 'NP'), ('함께', 'MAG'), ('열심히', 'MAG'), ('공부', 'NNG'), ('하', 'XSV'), ('ㅂ시다', 'EC')]\n",
            "['공부']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.3 **Okt 형태소**\n",
        "\n",
        "- 트위터에서 개발한 트위터 한국어 처리기에 파생된 오픈소스 한국어 처리기입니다.\n",
        "- 간단한 한국어 처리를 통해 색인어를 추출하는 목표를 갖고 있기에 완전한 수준의 형태소 분석을 지향하지 않습니다.\n",
        "- 띄어쓰기가 어느정도 되어 있는 문장을 빠르게 분석할 때 많이 사용합니다.\n",
        "- 불러오는 방법 : from konlpy.tag import Okt"
      ],
      "metadata": {
        "id": "eTTegbDc7Djq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Okt 모듈의 함수 설명**\n",
        "\n",
        "* morphs(phrase) : 인자로 입력한 문장을 형태소 단위로 토크나이징합니다. 토크나이징된 형태소들은 리스트 형태로 반환됩니다.\n",
        "* noun(phrase) : 인자로 입력한 문장에서 품사가 명사인 토큰들만 추출합니다.\n",
        "* pos(phrase, stem=Flase, join=False) : POS tagger라 부르며, 인자로 입력한 문장에서 형태소를 추출한 뒤 품사 태깅을 합니다. 추출된 형태소와 그 형태소의 품사가 튜플 형태로 묶여서 리스트로 반환됩니다.\n",
        "* normalize(phrase) : 입력한 문장을 정규화시킵니다.\n",
        "* phrases(phrase) : 입력한 문장에서 어구를 추출합니다."
      ],
      "metadata": {
        "id": "Q4y3gJgi7IFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**품사 태그표**\n",
        "\n",
        "* 명사 : Noun\n",
        "* 동사 : Verb\n",
        "* 형용사 : Adjective\n",
        "* 조사 : Josa\n",
        "* 구두점 : Punctuation\n"
      ],
      "metadata": {
        "id": "vjreqEjf5IEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**형태소 추출**\n"
      ],
      "metadata": {
        "id": "BAVRa57QAyw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Okt 형태소 분석기 객체 생성\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "okt=Okt()\n",
        "text=\"우리 함께 열심히 공부합시다.\"\n",
        "okt_morphs=okt.morphs(text)\n",
        "\n",
        "print(okt_morphs)"
      ],
      "metadata": {
        "id": "tExeIO63A06k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a578bf5-48d6-4d08-ba78-004388557449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['우리', '함께', '열심히', '공부', '합시다', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**형태소와 품사 태그 추출**\n",
        "\n",
        "text 변수에 저장된 문장을 품사 태깅합니다. 태깅된 품사와 형태소들은 리스트 형태로 반환됩니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jx0wYoqEA4rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt_pos=okt.pos(text)\n",
        "print(okt_pos)\n",
        "\n",
        "# 명사만 추출\n",
        "okt_noun=okt.nouns(text)\n",
        "print(okt_noun)"
      ],
      "metadata": {
        "id": "mQBccbY1A8po",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d03b10a-33b1-401b-afd8-9ac7c2ddf3a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('우리', 'Noun'), ('함께', 'Adverb'), ('열심히', 'Adverb'), ('공부', 'Noun'), ('합시다', 'Verb'), ('.', 'Punctuation')]\n",
            "['우리', '공부']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**정규화, 어구 추출**\n",
        "\n",
        "Okt는 앞서 소개한 형태소 분석기보다 분석되는 품사 정보는 작지만 분석 속도는 제일 빠릅니다. 또한 normalize() 함수를 지원해 오타가 섞인 문장을 정규화해서 처리하기 효율적입니다."
      ],
      "metadata": {
        "id": "yPP4CD9TA_w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='이것은 정규화하기 위한 예시입니닼ㅋㅋ오늘 날씨가 좋아요'\n",
        "print(okt.normalize(text))\n",
        "print(okt.phrases(text))"
      ],
      "metadata": {
        "id": "qeMwPxGhBDfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc77e3f5-469e-4b90-d9ea-616400d15584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이것은 정규화하기 위한 예시입니다ㅋㅋ오늘 날씨가 좋아요\n",
            "['이것', '정규화', '예시', '오늘', '오늘 날씨', '날씨']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**형태소 분석기 비교**\n",
        "\n",
        "Komoran\n",
        "\n",
        "- 장점 : 자소가 분리된 문장이나 오탈자에 강합니다. 사용사 사전 관리가 용이합니다.\n",
        "- 단점 : 적당한 분석 품질과 분석 속도\n",
        "\n",
        "Okt\n",
        "\n",
        "- 장점 : 매우 빠른 분석 속도, 오타 수정과 정규화 기능까지 지원합니다.\n",
        "- 단점 : 사용자 사전 관리가 어렵고 용언 분석에 일관성이 부족합니다."
      ],
      "metadata": {
        "id": "u8T7pItU7zbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.4 사용자 사전 구축\n",
        "\n",
        "자주 사용하지 않는 단어나 문장은 **형태소 분석기에서 인식이 안 되는 경우**가 많기 때문에 대부분의 형태소 분석기들은 **사용자가 직접 사전을 추가**할 수 있는 기능을 가지고 있습니다. \n",
        "\n",
        "\n",
        "**사용자 사전 등록 방법**\n",
        "\n",
        "user_div.tsv 파일로 저장\n",
        "\n",
        "```python\n",
        "# [단어] Tab [품사] 형태로 등록\n",
        "엔엘피 Tab NNG\n",
        "```"
      ],
      "metadata": {
        "id": "U1oV-qJx72NO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 사용자 사전 등록 전\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7hjuWKD374Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran()\n",
        "text = \"우리 챗봇은 **엔엘피**를 좋아해\"\n",
        "pos = komoran.pos(text)\n",
        "\n",
        "print(pos)"
      ],
      "metadata": {
        "id": "paYFiKEMBLIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5e04e4-2476-460a-d224-e6340e96f29a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('우리', 'NP'), ('챗봇은', 'NA'), ('*', 'SW'), ('*', 'SW'), ('엔', 'NNB'), ('엘', 'NNP'), ('피', 'NNG'), ('*', 'SW'), ('*', 'SW'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 사용자 사전 등록 후"
      ],
      "metadata": {
        "id": "L_wm73a-79Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/keiraydev/chatbot/master/book_ex/ch3/user_dic.tsv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_rc74UZUzuS",
        "outputId": "6e0f6031-e4da-4412-a028-40034587cbf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-15 10:08:42--  https://raw.githubusercontent.com/keiraydev/chatbot/master/book_ex/ch3/user_dic.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66 [text/plain]\n",
            "Saving to: ‘user_dic.tsv.1’\n",
            "\n",
            "\ruser_dic.tsv.1        0%[                    ]       0  --.-KB/s               \ruser_dic.tsv.1      100%[===================>]      66  --.-KB/s    in 0s      \n",
            "\n",
            "2022-07-15 10:08:42 (4.96 MB/s) - ‘user_dic.tsv.1’ saved [66/66]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "\n",
        "komoran = Komoran(userdic = './user_dic.tsv')\n",
        "text = \"우리 챗봇은 **엔엘피**를 좋아해\"\n",
        "pos = komoran.pos(text)\n",
        "\n",
        "print(pos)"
      ],
      "metadata": {
        "id": "lKzluwmxBh2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce32f925-b7ab-4cff-921e-1b7f0e5afcda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('우리', 'NP'), ('챗봇은', 'NA'), ('*', 'SW'), ('*', 'SW'), ('엔엘피', 'NNG'), ('*', 'SW'), ('*', 'SW'), ('를', 'JKO'), ('좋아하', 'VV'), ('아', 'EC')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 마치며\n",
        "\n",
        "전처리 (*preprocessing)은* 추출된 정보에서 필요 없는 정보를 제거하는 과정으로 문장의 의미와 의도를 분석하는 역할을 합니다. \n",
        "\n",
        "**KoNLPy 형태소 분석기 비교**\n",
        "\n",
        "| 형태소 분석기 | 장점 | 단점 |\n",
        "| --- | --- | --- |\n",
        "| Kkma | 분석 품질 우수<br/>지원하는 품사 수 다양 | 느린 분석 속도<br/>사용자 사전에 추가한 복합 명사 분석<br/>불완전 |\n",
        "| Komoran | 오탈자에 강함<br/>사용자 사전 관리 용이 |적당한 분석 품질과 속도 |\n",
        "| Okt | 빠른 분석 속도<br/>정규화 기능 | 사용자 사전 관리의 어려움<br/>용언 분석에 일관성 부족 |"
      ],
      "metadata": {
        "id": "JJOJdJeA79Yg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 임베딩\n",
        "## 2.1 임베딩이란 \n",
        "**단어나 문장을 수치화해 벡터 공간으로 표현하는 과정**을 가리킵니다.\n",
        "\n",
        "**특징**\n",
        "- 딥러닝 모델의 **입력값**으로 많이 사용됩니다.\n",
        "- 말뭉치의 의미에 따라 벡터화하기 때문에 **문법적인 정보**를 포함합니다.\n",
        "\n",
        "**기법**\n",
        "\n",
        "임베딩 기법에는 문장 임베딩과 단어 임베딩, 두 가지가 있습니다. \n",
        "\n",
        "|  | 특징 | 장점 | 단점 |\n",
        "| --- | --- | --- | --- |\n",
        "| 1. 문장 임베딩 | 문장 전체를 벡터로 표현 | 문맥적 의미를 지님 <br/>→ 품질이 좋음 <br/>→ 상용 시스템에 많이 사용됨 | → 임베딩하기 위해 많은 문장 데이터 필요<br/>→ 학습하는 데 높은 비용 소요 |\n",
        "| 2. 단어 임베딩<br/>(이 책에서 다룸) | 개별 단어를 벡터로 표현 | 학습 방법이 간단<br/>→ 실무에서 많이 사용 | 의미가 다르더라도 단어의 형태가 같다면 (동음이의어) 동일한 벡터값으로 표현  |"
      ],
      "metadata": {
        "id": "hC03CsZn4FYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 단어 임베딩\n",
        "\n",
        "추출된 형태소 기반 토큰들을 어떻게 단어 임베딩할까요?\n",
        "\n",
        "### 2.2.1 원-핫 인코딩\n",
        "\n",
        "원-핫 인코딩이란 **단 하나의 값만 1이고 나머지 요솟값은 0인 인코딩**입니다. 즉, 결과값이 **원-핫 벡터(i.e.,** **희소 sparse 벡터)**인 인코딩이죠.\n",
        "\n",
        "- 방법\n",
        "    1. **사전**(말뭉치에서 나오는 서로 다른 모든 단어의 집합)을 만듭니다.\n",
        "    2. 사전 내 단어 순서대로 고유한 **인덱스 번호**를 부여합니다.\n",
        "    3. 인덱스 번호가 **원-핫 인코딩에서 1의 값을 가지는 요소의 위치**가 됩니다.\n",
        "    \n"
      ],
      "metadata": {
        "id": "SUd-EJLs8Tif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Komoran\n",
        "import numpy as np\n",
        "\n",
        "komoran = Komoran()\n",
        "text = \"오늘 날씨는 구름이 많아요.\"\n",
        "\n",
        "# 문장에서 명사만 추출\n",
        "nouns = komoran.nouns(text) # 오늘, 날씨, 구름\n",
        "\n",
        "# 단어 사전 구축 및 단어별 인덱스 부여\n",
        "dics = {}\n",
        "for word in nouns:\n",
        "  if word not in dics.keys(): # 이미 저장된 단어는 다시 저장하지 않음\n",
        "    dics[word] = len(dics) # key가 단어, value가 인덱스\n",
        "\n",
        "# 원-핫 벡터 차원의 크기는 단어 사전의 크기\n",
        "nb_classes = len(dics)\n",
        "\n",
        "# 단어별 인덱스값을 리스트로 변환\n",
        "targets = list(dics.values())\n",
        "\n",
        "# 원-핫 벡터 생성\n",
        "one_hot_targets = np.eye(nb_classes) # nb_classes 크기대로 단위행렬을 반환\n",
        "one_hot_targets = one_hot_targets[targets] # 단위행렬을 단어 사전의 순서에 맞게 정렬\n",
        "print(one_hot_targets)"
      ],
      "metadata": {
        "id": "JfsZpQdcBnxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22ed2c57-fabe-4ce4-e9c5-fc89100c44ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- 한계\n",
        "    - 단어의 의미나 유사한 단어와의 관계를 담고 있지 않습니다.\n",
        "    - 단어 사전의 크기가 커짐에 따라 원-핫 벡터의 차원도 커져 메모리 낭비와 계산의 복잡도가 커집니다.\n",
        "    - 대부분의 요소가 0의 값을 가지고 있으므로 비효율적입니다."
      ],
      "metadata": {
        "id": "OSv9P5GBBp1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 희소 표현과 분산 표현 \n",
        "\n",
        "**희소 표현이란?**\n",
        "\n",
        "앞서 소개한 원-핫 인코딩처럼 단어의 인덱스 요소만 1이고 나머지 요소는 모두 0으로 표현되는 **희소 벡터(또는 희소 행렬)**로 표현되는 방식입니다.\n",
        "\n",
        "- 장점: 각각의 차원이 독립적인 정보를 지니고 있어 사람이 이해하기에 직관적입니다.\n",
        "- 단점: 단어 사전의 크기가 커질수록 메모리 낭비와 계산 복잡도가 커지고, 단어 간의 연관성이 전혀 없어 의미를 담을 수 없습니다.\n",
        "\n",
        "**분산 표현이란?**\n",
        "\n",
        "단어의 의미와 주변 단어 간의 관계를 잘 표현하기 위해 고안된 방식으로, **단어 간의 유사성을 잘 표현**하면서도 **벡터 공간을 절약**할 수 있는 방법입니다.\n",
        "\n",
        "- 특징\n",
        "    1. 하나의 차원에 다양한 정보를 가지고 있습니다.\n",
        "        \n",
        "        Ex) 색상을 표현하는 RGB 모델 → 예를 들어 연두색은 RGB(204, 255, 204) 형태로 분산 표현을 하는 것이죠. \n",
        "        (신경망 모델에서의 분산 표현은 사람이 직관적으로 알 수 없는 형태의 수치로 표현됩니다.)\n",
        "        \n",
        "    2. 희소 표현에 비해 장점이 많아 단어 임베딩 기법에서 가장 많이 사용되고 있는 방식입니다.\n",
        "\n",
        "신경망에서는 분산 표현 학습하는 과정에서 임베딩 벡터의 모든 차원에 의미 있는 데이터를 고르게 밀집시켜 데이터 손실을 최소화하면서 벡터 차원이 압축됩니다. 이 때문에 분산 표현 방식을 **밀집 표현**이라 부르기도 하고, 밀집 표현으로 만들어진 벡터를 **밀집 벡터**라 합니다.\n",
        "\n",
        "**분산 표현 방식의 장점**\n",
        "\n",
        "1. 임베딩 벡터의 차원을 데이터 손실을 최소화하면서 압축할 수 있습니다.\n",
        "    - 입력 데이터의 차원이 너무 높아지는 신경망 모델의 학습이 어려워지는 차원의 저주(curse of dimensionality) 문제에 유연하게 처리가 가능합니다. (100~200차원 정도만 사용해도 많은 단어를 표현할 수 있다.)\n",
        "2. 임베딩 벡터에는 단어의 의미, 주변 단어 간의 관계 등 많은 정보가 내포되어 있어 일반화 능력이 뛰어납니다.\n",
        "    - 벡터 공간 상에서 유사한 의미를 갖는 단어들은 비슷한 위치에 분포되어 있어 두 단어 간의 거리 계산을 통해 유사성을 판별할 수 있습니다."
      ],
      "metadata": {
        "id": "vUj3p1Ty8b6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 Word2Vec\n",
        "\n",
        "분산 표현 형태의 신경망 기반 단어 임베딩의 대표적인 방법인 Word2Vec에 대해 알아보겠습니다.\n",
        "\n",
        "**Word2Vec이란?**\n",
        "\n",
        "2013년 구글에서 발표했으며 가장 많이 사용하고 있는 단어 임베딩 모델로, 기존 신경망 기반의 단어 임베딩 모델에 비해 구조상 큰 차이는 없지만 **계산량을 획기적으로 줄여 빠른 학습이 가능**한 장점이 있습니다.\n",
        "\n",
        "**Word2Vec의 두 가지 모델**\n",
        "\n",
        "1. **CBOW(Continuous Bag-Of-Words)**\n",
        "\n",
        "    - CBOW 모델은 맥락이라 표현되는 **주변 단어들을 이용해 타깃 단어를 예측**하는 신경망 모델입니다.\n",
        "\n",
        "    - 장점: 타깃 단어의 손실만 계산하면 되기 때문에 학습 속도가 빠릅니다.\n",
        "\n",
        "2. **skip-gram**\n",
        "\n",
        "    - CBOW 모델과 반대로 **하나의 타깃 단어를 이용해 주변 단어들을 예측**하는 신경망 모델입니다.\n",
        "\n",
        "    - 장점: CBOW 모델에 비해 예측해야 하는 맥락이 많기 때문에 단어 분산 표현력이 우수해 임베딩 품질도 우수합니다. \n",
        "\n",
        "**윈도우(window)**\n",
        "\n",
        "CBOW 모델에서 타깃 단어를 예측하기 위해 앞뒤 단어를 확인하는데, 이때 앞뒤로 몇 개의 단어까지 확인할지 결정하는 범위를 윈도우라고 합니다.\n",
        "\n",
        "Ex) window size=2 일 때, 타깃 단어의 앞뒤 단어 2개까지 모델 학습을 위해 사용됨"
      ],
      "metadata": {
        "id": "Zi0PSxXr8j9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec 파이썬 예제**\n",
        "\n",
        "이제 Word2Vec 모델을 파이썬 예제를 통해 사용해봅시다.\n",
        "\n",
        "- 라이브러리: **Gensim** 패키지 (Word2Vec을 사용할 수 있는 오픈소스 라이브러리 이미 존재함)\n",
        "- 데이터: 네이버 영화 리뷰 데이터(Naver Sentiment Movie Corpus, NSMC)\n",
        "    \n",
        "    [최신 버전의 말뭉치 데이터 받기](https://github.com/e9t/nsmc)\n",
        "    \n",
        "\n",
        "말뭉치 데이터의 양이 많기 때문에 학습하는 데 시간이 다소 걸립니다. 모델 학습을 매번 할 수 없으므로 각 단어의 임베딩 벡터가 설정되어 있는 모델을 파일로 저장해줍니다.\n",
        "\n",
        "예제로 제공된 ratings.txt 파일(네이버 영화 리뷰 말뭉치 데이터)과 동일한 위치에 create_word2vec_model.py 파일을 생성해 예제를 작성합니다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b8O2abBi8n1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_bFtO89blWg",
        "outputId": "c5936813-ce57-4b28-daa1-b6a9bcc89b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-15 10:08:59--  https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19515078 (19M) [text/plain]\n",
            "Saving to: ‘ratings.txt’\n",
            "\n",
            "ratings.txt         100%[===================>]  18.61M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-07-15 10:09:01 (130 MB/s) - ‘ratings.txt’ saved [19515078/19515078]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from konlpy.tag import Komoran\n",
        "import time\n",
        "\n",
        "# 네이버 영화 리뷰 데이터 읽어옴\n",
        "def read_review_data(filename):\n",
        "    with open(filename, 'r', encoding=\"utf-8\") as f:\n",
        "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
        "        data = data[1:] # 헤더 제거\n",
        "    return data\n",
        "\n",
        "# 학습 시간 측정 시작\n",
        "start = time.time()\n",
        "\n",
        "# 리뷰 파일 읽어오기\n",
        "print('1) 말뭉치 데이터 읽기 시작')\n",
        "review_data = read_review_data('./ratings.txt')\n",
        "print(len(review_data)) # 리뷰 데이터 전체 개수\n",
        "print('1) 말뭉치 데이터 읽기 완료 : ', time.time() - start)\n",
        "\n",
        "# 문장 단위로 명사만 추출해 학습 입력 데이터로 만듦\n",
        "print('2) 형태소에서 명사만 추출 시작')\n",
        "komoran = Komoran()\n",
        "docs = [komoran.nouns(sentence[1]) for sentence in review_data]\n",
        "print('2) 형태소에서 명사만 추출 완료 : ', time.time() - start)\n",
        "\n",
        "# Word2Vec 모델 학습\n",
        "print('3) Word2Vec 모델 학습 시작')\n",
        "model = Word2Vec(sentences=docs, size=200, window=4, hs=1, min_count=2, sg=1)\n",
        "print('3) Word2Vec 모델 학습 완료 : ', time.time() - start)\n",
        "\n",
        "# 모델 저장\n",
        "print('4) 학습된 모델 저장 시작')\n",
        "model.save('nvmc.model')\n",
        "print('4) 학습된 모델 저장 완료 : ', time.time() - start)\n",
        "\n",
        "# 학습된 말뭉치 수, 코퍼스 내 전체 단어 수\n",
        "print(\"corpus_count : \", model.corpus_count)\n",
        "print(\"corpus_total_words : \", model.corpus_total_words)"
      ],
      "metadata": {
        "id": "QuAm2uPiBypA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d1c7ba-8320-4f77-8730-4e1094779d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) 말뭉치 데이터 읽기 시작\n",
            "200000\n",
            "1) 말뭉치 데이터 읽기 완료 :  3.6582343578338623\n",
            "2) 형태소에서 명사만 추출 시작\n",
            "2) 형태소에서 명사만 추출 완료 :  207.93431091308594\n",
            "3) Word2Vec 모델 학습 시작\n",
            "3) Word2Vec 모델 학습 완료 :  245.7606611251831\n",
            "4) 학습된 모델 저장 시작\n",
            "4) 학습된 모델 저장 완료 :  247.8189995288849\n",
            "corpus_count :  200000\n",
            "corpus_total_words :  1076896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word2Vec의 주요 하이퍼파라미터**\n",
        "\n",
        "- sentences: Word2Vec 모델 학습에 필요한 문장 데이터로 Word2Vec 모델의 입력값으로 사용됩니다.\n",
        "- size: 단어 임베딩 벡터의 차원(크기)\n",
        "- window: 주변 단어 윈도우의 크기\n",
        "- hs: 1(모델학습에 softmax 사용) / 0(negative 옵션 값이 0이 아닐 때 음수 샘플링으로 사용)\n",
        "    - 음수 샘플링: 학습 과정에서 전체 단어 집합의 임베딩 벡터를 업데이트하지 않고 일부 단어 집합만 업데이트하는 방법 ⇒ 단어 개수가 많아질수록 계산 복잡도가 증가하여 연산 속도가 저하된다는 한계점을 보완\n",
        "    - 소프트맥스는 빈도가 낮은 단어에 더 잘 작동하고, 음수 샘플링은 빈도가 높은 단어와 저차원 벡터에 더 잘 작동합니다.\n",
        "- min_count: 단어 최소 빈도 수 제한(설정된 min_count 빈도 수 이하의 단어들은 학습하지 않음)\n",
        "- sg: 0(CBOW 모델) / 1(skip-gram 모델)"
      ],
      "metadata": {
        "id": "qC-6gorF8qIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 생성된 Word2Vec 모델 파일을 읽어와 실제로 단어 임베딩된 값과 벡터 공간상의 유사한 단어들을 확인하는 예제를 살펴봅시다.\n",
        "\n"
      ],
      "metadata": {
        "id": "X7LQEjjH8szH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 모델 로딩\n",
        "model = Word2Vec.load('nvmc.model')\n",
        "print(\"corpus_total_words : \", model.corpus_total_words)\n",
        "\n",
        "# '사랑'이란 단어로 생성한 단어 임베딩 벡터\n",
        "print('사랑 : ', model.wv['사랑'])\n",
        "\n",
        "# 단어 유사도 계산\n",
        "print(\"일요일 = 월요일\\t\", model.wv.similarity(w1='일요일', w2='월요일'))\n",
        "print(\"안성기 = 배우\\t\", model.wv.similarity(w1='안성기', w2='배우'))\n",
        "print(\"대기업 = 삼성\\t\", model.wv.similarity(w1='대기업', w2='삼성'))\n",
        "print(\"일요일 != 삼성\\t\", model.wv.similarity(w1='일요일', w2='삼성'))\n",
        "print(\"히어로 != 삼성\\t\", model.wv.similarity(w1='히어로', w2='삼성'))\n",
        "\n",
        "# 가장 유사한 단어 추출\n",
        "print(model.wv.most_similar('안성기', topn=5))\n",
        "print(model.wv.most_similar('시리즈', topn=5))"
      ],
      "metadata": {
        "id": "TzfHKV0TB2UL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3819b98-37ea-43ba-c58c-ddc3e8684ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus_total_words :  1076896\n",
            "사랑 :  [-0.3432236   0.01241295 -0.28553864  0.09851086 -0.3275371   0.27603108\n",
            " -0.12190133  0.23028255  0.06462078 -0.05123152 -0.29359406  0.08366198\n",
            " -0.22862822 -0.08917548 -0.17962375  0.17669371  0.22727151 -0.12626874\n",
            " -0.06467948 -0.04811384 -0.10341004 -0.24429026  0.33885995 -0.38067967\n",
            " -0.12439684  0.11595307 -0.22988199 -0.10268238 -0.17653562 -0.20466836\n",
            "  0.26444674 -0.12070999  0.17851     0.06733174 -0.30269527 -0.07242263\n",
            " -0.21155232 -0.17850944 -0.11125495 -0.23779829 -0.219859   -0.29448915\n",
            " -0.0833392   0.02464269 -0.08253264  0.12491167 -0.03993247 -0.22789304\n",
            "  0.10574128  0.11032473  0.03201863  0.16688909 -0.12961212  0.15161206\n",
            " -0.54391694  0.10925089 -0.15094264 -0.09466483  0.04477878 -0.21426715\n",
            " -0.1140954  -0.34715107  0.00824425  0.30230457 -0.24886717  0.14002317\n",
            " -0.06358381  0.32763124  0.10827292 -0.07478845 -0.1136454  -0.19656777\n",
            " -0.30974898  0.39492977 -0.06137576 -0.44177195  0.5342463   0.02964574\n",
            "  0.19740778 -0.22313637  0.3427846   0.09961651 -0.47915572  0.11830376\n",
            " -0.09552811  0.07734866  0.19005921  0.05274796 -0.17176999 -0.09081568\n",
            "  0.05934374 -0.09675828  0.08161296 -0.10087359  0.05023064 -0.22432393\n",
            " -0.1050508   0.08827329 -0.30098516  0.19520502 -0.08155736  0.0047098\n",
            "  0.09498692  0.00900079  0.0895338  -0.27128306  0.07020004 -0.30098304\n",
            " -0.04558004  0.136067    0.12377794 -0.2155459   0.12857562  0.0174466\n",
            "  0.03136416 -0.09483042 -0.06472188 -0.13280135 -0.22744958  0.2476853\n",
            "  0.00395336 -0.23179877 -0.2048292   0.180255   -0.11306518  0.49674967\n",
            " -0.02300451 -0.03912727 -0.04976272  0.10743036 -0.0993893  -0.13993828\n",
            " -0.06066691 -0.16543102 -0.16024132  0.5340414   0.31208688  0.24664249\n",
            " -0.04429481 -0.07744998  0.00962096 -0.16785185  0.40786058 -0.0746619\n",
            " -0.07398954  0.11432877  0.00489482  0.2628548   0.27406573  0.17971015\n",
            " -0.04668249  0.02016117 -0.2395605   0.06136238 -0.08773566  0.01666527\n",
            " -0.4833143  -0.03121108  0.06113671 -0.0811367   0.22484371 -0.05385954\n",
            "  0.10133106 -0.27374858 -0.26942006  0.21077242  0.23419276  0.07406957\n",
            " -0.30716857  0.01115524  0.17521814  0.13036533 -0.46483055  0.20579001\n",
            " -0.32453325 -0.11677337  0.17591153 -0.20392735 -0.12717417  0.07280127\n",
            " -0.08928195  0.10952461  0.00590979  0.379218   -0.32252097 -0.00196321\n",
            " -0.37123626 -0.38424012  0.1155588   0.21930741 -0.118314   -0.16587488\n",
            " -0.34566638 -0.14397593 -0.16153911  0.04973935 -0.19047654 -0.20703083\n",
            "  0.03625657  0.08641915]\n",
            "일요일 = 월요일\t 0.70791334\n",
            "안성기 = 배우\t 0.53391683\n",
            "대기업 = 삼성\t 0.61774987\n",
            "일요일 != 삼성\t 0.28959662\n",
            "히어로 != 삼성\t 0.15117915\n",
            "[('정려원', 0.7475878000259399), ('씨야', 0.7148264050483704), ('장미희', 0.7063388824462891), ('박신양', 0.6980233192443848), ('이은우', 0.6975529193878174)]\n",
            "[('러시아워', 0.649056077003479), ('캐리비안의 해적', 0.6485556364059448), ('더 울버린', 0.6447342038154602), ('나니아 연대기', 0.6290414333343506), ('반지의 제왕', 0.6154429912567139)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- model.wv.similarity(): 두 단어 간의 유사도를 계산해줍니다.\n",
        "- model.wv.most_similar(): 인자로 사용한 단어와 가장 유사한 단어, 즉 벡터 공간에서 가장 가까운 거리게 있는 단어를 리스트로 반환해줍니다. 이때 topn 인자는 반환되는 단어 수를 의미합니다.\n",
        "\n",
        "\n",
        "\n",
        "실행 결과를 살펴보면, ‘사랑’의 단어 임베딩 벡터가 모든 차원에 골고루 데이터가 분포되어 있는 것을 볼 수 있습니다. ‘일요일’과 ‘월요일’에 대한 유사도를 계산한 결과를 보면 0.65로 수치가 높습니다. 이때 유사도가 1에 가까울 수록 두 단어는 동일한 의미이거나 문법적으로 관계되어 있을 확률이 높습니다.\n",
        "\n",
        "그 밖에도 ‘히어로’와 ‘삼성’의 유사도는 0.13으로 매우 낮고, 이는 서로 관계없는 단어일 확률이 높습니다.\n",
        "\n",
        "‘시리즈’와 가장 유사한 단어 5개를 찾았을 때도 시리즈물 영화 제목이 반환된 것을 확인할 수 있습니다.\n",
        "\n",
        "이렇듯 Word2Vec을 단어 임베딩으로 사용하면 단어의 의미나 문법적인 특징까지도 벡터로 표현할 수 있습니다. 간혹 이해하기 힘든 결과를 출력할 때도 있지만, 이는 주제에 맞는 더 많은 말뭉치 데이터를 학습하면 임베딩 성능이 많이 좋아집니다."
      ],
      "metadata": {
        "id": "95pkVfkBB7Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 마치며\n",
        "\n",
        "자연어를 컴퓨터 연산에 효율적인 백터 형태로 변환하는 과정을 **임베딩**이라고 하고, 이런 임베딩은 사람이 이해하는 정보를 **컴퓨터가 이해할 수 있는 형태로 변환**해주는 역할을 하기 때문에 일반적으로 **신경망 모델의 입력**으로 많이 사용됩니다.\n",
        "\n",
        "앞으로 만들어볼 챗봇은 단어 기반 토큰을 활용한 단어 임베딩 벡터를 사용합니다. 어떤 임베딩 방법을 사용하느냐에 따라 챗봇 엔진의 성능에도 많은 영향을 주기 때문에 매우 중요한 부분이라고 볼 수 있습니다."
      ],
      "metadata": {
        "id": "4IG-6BQt8vd8"
      }
    }
  ]
}